{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8d1db3c-6076-40bc-85f9-34d09badd3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Union, Tuple\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import instaloader\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import json \n",
    "from itertools import takewhile, dropwhile\n",
    "import requests\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from dateutil import parser as dateparser\n",
    "from newspaper import Article\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7356ecc4-46fd-4ba9-b90e-3ba386edd546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_profile(\n",
    "    usernames: Union[str, List[str]], root: os.PathLike = \"../data\", **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Downloads all posts of a given publicly-accessible profile.\n",
    "    Does not download images or videos, only metadata\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    usernames : str, List[str]\n",
    "        Username(s) of the profile to download\n",
    "    root : str\n",
    "        Path to the folder where the posts will be stored\n",
    "    \"\"\"\n",
    "\n",
    "    loader = instaloader.Instaloader(\n",
    "        dirname_pattern=os.path.join(root, \"{profile}\"),\n",
    "        download_pictures=False,\n",
    "        download_videos=False,\n",
    "        download_video_thumbnails=False,\n",
    "        download_geotags=False,\n",
    "        download_comments=False,\n",
    "        save_metadata=True,\n",
    "        compress_json=False,\n",
    "    )\n",
    "\n",
    "    if isinstance(usernames, str):\n",
    "        usernames = [usernames]\n",
    "\n",
    "    profiles = [\n",
    "        instaloader.Profile.from_username(loader.context, username)\n",
    "        for username in usernames\n",
    "    ]\n",
    "\n",
    "    latest_stamps = instaloader.LatestStamps(os.path.join(root, \"latest_timestamp.txt\"))\n",
    "\n",
    "    loader.download_profiles(\n",
    "        profiles,\n",
    "        fast_update=True,\n",
    "        profile_pic=False,\n",
    "        igtv=False,\n",
    "        latest_stamps=latest_stamps,\n",
    "        stories=False,\n",
    "        highlights=False,\n",
    "        tagged=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e325fef-6539-41cf-bb1f-eb91ede22bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1] Downloading profile pubitysport\n",
      "Retrieving posts from profile pubitysport.\n",
      "[    4/12636] [SWIPE âž¡ Barcelona's new moneyâ€¦] json \n",
      "[    5/12636] [Spurs' 24 minute implosion:  â€¦] json \n",
      "[    6/12636] [ð–ð‡ð€ð“ ð€ ð†ð€ðŒð„ ðŸ¤¯ðŸ¿  â€” ð—™ð—œð—©ð—˜ goals â€¦] json \n",
      "[    7/12636] [AT LAST CHELSEA SCORE, NICHOLâ€¦] json \n",
      "[    8/12636] [SPURS ARE NOW DOWN TO 9 MEN. â€¦] json \n",
      "[    9/12636] [ROMERO CONCEDES A PENALTY, GEâ€¦] json \n",
      "[   10/12636] [Tottenham Hotspur's Destiny Uâ€¦] json \n",
      "[   11/12636] [CHELSEA GO 1-0 DOWN WITHIN THâ€¦] json \n"
     ]
    }
   ],
   "source": [
    "download_profile(\"pubitysport\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "513d2fa6-e937-46b4-8899-bb431fd08ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer:\n",
    "    \"\"\"\n",
    "    A class for vectorizing text inputs\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, how: str = \"tfidf\", ngram_range: Tuple[int, int] = (1, 1)):\n",
    "        \"\"\"\n",
    "        Initializes the vectorizer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        how : str\n",
    "            How to vectorize the content. Can be either \"tfidf\", \"bow\" (bag of words), or \"bert\"\n",
    "        ngram_range : Tuple[int, int]\n",
    "            Range of ngrams to use for tfidf or count vectorization\n",
    "        \"\"\"\n",
    "        self.how = how\n",
    "        self.ngram_range = ngram_range\n",
    "        if self.how in [\"tfidf\", \"bow\"]:\n",
    "            self.vectorizer = (\n",
    "                TfidfVectorizer\n",
    "                if self.how == \"tfidf\"\n",
    "                else CountVectorizer\n",
    "            )(\n",
    "                input=\"filename\",\n",
    "                strip_accents=\"unicode\",\n",
    "                ngram_range=self.ngram_range,\n",
    "            )\n",
    "        elif self.how == \"bert\":\n",
    "            self.vectorizer = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                \"vinai/bertweet-base\", use_fast=False\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(\"how must be either tfidf, bow or bert\")\n",
    "        self.trained = False\n",
    "        \n",
    "    def fit_transform(self, text_files: List[os.PathLike], batch_size: int = 8) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Fits the vectorizer to the given text files, and returns the vectors\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text_files : List[os.PathLike]\n",
    "            List of paths to the text files to fit the vectorizer to\n",
    "        batch_size : int\n",
    "            Batch size for bert vectorization\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Array containing the vectors\n",
    "        \"\"\"\n",
    "        if self.how in [\"tfidf\", \"bow\"]:\n",
    "            vectors = self.vectorizer.fit_transform(text_files)\n",
    "        elif self.how == \"bert\":\n",
    "            all_embeddings = []\n",
    "\n",
    "            for i in range(0, len(text_files), 8):\n",
    "                print(text_files[i:i + 8])\n",
    "\n",
    "                batch_contents = [open(file_path, 'r', encoding='utf-8').read() for file_path in text_files[i:i + 8] if os.path.exists(file_path)]\n",
    "\n",
    "                tokens = self.tokenizer(batch_contents, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.vectorizer(**tokens)\n",
    "                    embeddings = [o.numpy() for o in outputs.last_hidden_state]  # This contains the embeddings for each token in the input\n",
    "                    all_embeddings.extend(embeddings)\n",
    "            \n",
    "            vectors = np.array(all_embeddings)\n",
    "        else:\n",
    "            raise NotImplementedError(\"how must be either tfidf, bow or bert\")\n",
    "        \n",
    "        self.trained = True\n",
    "        return vectors\n",
    "    \n",
    "    def transform(self, text: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Transforms the given text into a vector\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text : str\n",
    "            Text to transform\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Array containing the vector\n",
    "        \"\"\"\n",
    "        if not self.trained:\n",
    "            raise ValueError(\"Vectorizer must be trained first\")\n",
    "        if self.how in [\"tfidf\", \"bow\"]:\n",
    "            vector = self.vectorizer.transform([text])\n",
    "        elif self.how == \"bert\":\n",
    "            tokens = self.tokenizer([text], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.vectorizer(**tokens)\n",
    "                vector = outputs.last_hidden_state[0].numpy()\n",
    "\n",
    "        return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "78540c38-909d-4077-ac19-0ed786c3096c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /Users/johnsutor/anaconda3/envs/nlp-project/lib/python3.11/site-packages (4.66.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "16d620c0-7168-49b5-8678-841fa2f69c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/pubitysport/2023-06-19_20-02-07_UTC.txt', '../data/pubitysport/2019-06-29_20-32-58_UTC.txt', '../data/pubitysport/2020-02-24_14-58-57_UTC.txt', '../data/pubitysport/2021-10-16_12-43-27_UTC.txt', '../data/pubitysport/2019-05-11_10-50-10_UTC.txt', '../data/pubitysport/2020-05-04_16-19-04_UTC.txt', '../data/pubitysport/2022-08-19_09-53-27_UTC.txt', '../data/pubitysport/2018-04-22_19-20-46_UTC.txt']\n",
      "['../data/pubitysport/2019-09-06_18-43-47_UTC.txt', '../data/pubitysport/2019-07-22_11-21-36_UTC.txt', '../data/pubitysport/2023-03-06_17-36-27_UTC.txt', '../data/pubitysport/2020-10-04_19-14-11_UTC.txt', '../data/pubitysport/2022-06-14_10-30-45_UTC.txt', '../data/pubitysport/2021-12-03_22-25-20_UTC.txt', '../data/pubitysport/2019-03-23_12-13-24_UTC.txt', '../data/pubitysport/2023-06-14_19-02-45_UTC.txt']\n",
      "['../data/pubitysport/2023-02-04_11-46-07_UTC.txt', '../data/pubitysport/2021-07-07_19-46-53_UTC.txt', '../data/pubitysport/2023-09-10_18-47-37_UTC.txt', '../data/pubitysport/2018-12-23_16-05-11_UTC.txt', '../data/pubitysport/2022-10-05_11-12-35_UTC.txt', '../data/pubitysport/2020-04-14_16-54-34_UTC.txt', '../data/pubitysport/2020-04-10_08-46-05_UTC.txt', '../data/pubitysport/2022-02-17_11-51-22_UTC.txt']\n",
      "['../data/pubitysport/2020-07-11_18-42-10_UTC.txt', '../data/pubitysport/2020-09-06_13-30-43_UTC.txt', '../data/pubitysport/2019-11-17_16-51-16_UTC.txt', '../data/pubitysport/2020-03-24_15-40-01_UTC.txt', '../data/pubitysport/2019-09-17_11-44-19_UTC.txt', '../data/pubitysport/2019-06-20_20-15-59_UTC.txt', '../data/pubitysport/2021-12-13_21-17-28_UTC.txt', '../data/pubitysport/2021-09-13_15-07-34_UTC.txt']\n",
      "['../data/pubitysport/2020-11-19_16-13-35_UTC.txt', '../data/pubitysport/2020-04-13_15-30-24_UTC.txt', '../data/pubitysport/2021-10-02_19-56-37_UTC.txt', '../data/pubitysport/2019-02-03_22-48-43_UTC.txt', '../data/pubitysport/2020-09-20_16-19-59_UTC.txt', '../data/pubitysport/2020-11-19_16-26-10_UTC.txt', '../data/pubitysport/2023-08-13_18-51-38_UTC.txt', '../data/pubitysport/2018-08-29_17-35-41_UTC.txt']\n",
      "['../data/pubitysport/2023-09-11_15-46-43_UTC.txt', '../data/pubitysport/2021-08-04_20-39-28_UTC.txt', '../data/pubitysport/2020-10-16_13-27-02_UTC.txt', '../data/pubitysport/2023-05-25_18-42-34_UTC.txt', '../data/pubitysport/2023-02-06_17-48-37_UTC.txt', '../data/pubitysport/2019-08-29_17-21-52_UTC.txt', '../data/pubitysport/2020-12-05_11-02-06_UTC.txt', '../data/pubitysport/2020-05-01_15-32-53_UTC.txt']\n",
      "['../data/pubitysport/2018-12-07_16-13-55_UTC.txt', '../data/pubitysport/2020-10-26_13-38-41_UTC.txt', '../data/pubitysport/2018-12-13_01-37-02_UTC.txt', '../data/pubitysport/2019-07-28_20-04-19_UTC.txt', '../data/pubitysport/2022-01-25_21-07-28_UTC.txt', '../data/pubitysport/2021-07-25_18-50-32_UTC.txt', '../data/pubitysport/2019-12-13_20-44-17_UTC.txt', '../data/pubitysport/2018-10-21_16-40-00_UTC.txt']\n",
      "['../data/pubitysport/2020-10-22_20-52-53_UTC.txt', '../data/pubitysport/2019-08-15_13-17-43_UTC.txt', '../data/pubitysport/2020-10-21_19-13-24_UTC.txt', '../data/pubitysport/2020-06-18_17-31-00_UTC.txt', '../data/pubitysport/2020-06-28_18-37-18_UTC.txt', '../data/pubitysport/2019-10-20_11-29-49_UTC.txt', '../data/pubitysport/2018-05-31_15-41-59_UTC.txt', '../data/pubitysport/2018-02-14_20-27-05_UTC.txt']\n",
      "['../data/pubitysport/2023-03-17_22-07-01_UTC.txt', '../data/pubitysport/2023-08-17_22-38-13_UTC.txt', '../data/pubitysport/2018-12-22_16-05-56_UTC.txt', '../data/pubitysport/2020-05-27_09-31-44_UTC.txt', '../data/pubitysport/2019-02-02_12-41-10_UTC.txt', '../data/pubitysport/2021-07-21_13-09-04_UTC.txt', '../data/pubitysport/2018-11-06_13-34-08_UTC.txt', '../data/pubitysport/2019-05-22_16-50-12_UTC.txt']\n",
      "['../data/pubitysport/2019-10-20_20-38-24_UTC.txt', '../data/pubitysport/2020-05-10_12-11-15_UTC.txt', '../data/pubitysport/2023-10-09_14-34-11_UTC.txt', '../data/pubitysport/2023-04-09_18-38-03_UTC.txt', '../data/pubitysport/2020-07-19_19-12-36_UTC.txt', '../data/pubitysport/2019-02-14_15-12-02_UTC.txt', '../data/pubitysport/2018-10-31_13-37-35_UTC.txt', '../data/pubitysport/2019-10-30_00-46-08_UTC.txt']\n",
      "['../data/pubitysport/2020-04-19_17-43-41_UTC.txt', '../data/pubitysport/2020-06-01_19-07-56_UTC.txt', '../data/pubitysport/2021-10-27_20-58-23_UTC.txt', '../data/pubitysport/2019-11-10_14-38-03_UTC.txt', '../data/pubitysport/2019-01-31_20-19-45_UTC.txt', '../data/pubitysport/2020-05-13_22-39-21_UTC.txt', '../data/pubitysport/2019-06-23_20-07-23_UTC.txt', '../data/pubitysport/2022-07-24_18-22-09_UTC.txt']\n",
      "['../data/pubitysport/2022-08-28_18-31-32_UTC.txt', '../data/pubitysport/2023-01-20_21-52-17_UTC.txt', '../data/pubitysport/2020-03-18_19-23-39_UTC.txt', '../data/pubitysport/2023-09-29_12-10-58_UTC.txt', '../data/pubitysport/2023-04-26_14-48-12_UTC.txt', '../data/pubitysport/2022-01-01_15-08-48_UTC.txt', '../data/pubitysport/2018-10-28_21-36-32_UTC.txt', '../data/pubitysport/2020-11-01_16-00-15_UTC.txt']\n",
      "['../data/pubitysport/2020-09-30_13-22-30_UTC.txt', '../data/pubitysport/2019-06-24_13-23-08_UTC.txt', '../data/pubitysport/2021-02-21_18-59-57_UTC.txt', '../data/pubitysport/2018-07-05_15-06-34_UTC.txt', '../data/pubitysport/2021-07-29_21-50-27_UTC.txt', '../data/pubitysport/2019-06-11_19-01-59_UTC.txt', '../data/pubitysport/2018-11-17_21-16-03_UTC.txt', '../data/pubitysport/2020-08-17_10-37-16_UTC.txt']\n",
      "['../data/pubitysport/2021-09-28_20-52-39_UTC.txt', '../data/pubitysport/2020-08-28_14-33-34_UTC.txt', '../data/pubitysport/2018-02-12_08-05-00_UTC.txt', '../data/pubitysport/2023-11-05_19-30-19_UTC.txt', '../data/pubitysport/2018-11-29_23-31-01_UTC.txt', '../data/pubitysport/2018-11-04_11-25-34_UTC.txt', '../data/pubitysport/2023-09-01_12-26-32_UTC.txt', '../data/pubitysport/2018-05-05_18-58-41_UTC.txt']\n",
      "['../data/pubitysport/2023-02-09_14-24-14_UTC.txt', '../data/pubitysport/2018-12-14_11-55-52_UTC.txt', '../data/pubitysport/2023-06-20_14-28-07_UTC.txt', '../data/pubitysport/2021-08-05_10-54-04_UTC.txt', '../data/pubitysport/2023-11-02_13-39-10_UTC.txt', '../data/pubitysport/2019-06-25_18-11-14_UTC.txt', '../data/pubitysport/2022-07-13_19-07-21_UTC.txt', '../data/pubitysport/2018-12-02_23-06-24_UTC.txt']\n",
      "['../data/pubitysport/2018-06-23_22-12-14_UTC.txt', '../data/pubitysport/2023-06-06_20-20-36_UTC.txt', '../data/pubitysport/2019-01-05_19-18-50_UTC.txt', '../data/pubitysport/2021-09-15_15-33-11_UTC.txt', '../data/pubitysport/2021-08-21_09-59-32_UTC.txt', '../data/pubitysport/2023-07-26_08-34-06_UTC.txt', '../data/pubitysport/2020-07-22_14-20-26_UTC.txt', '../data/pubitysport/2019-04-08_13-36-00_UTC.txt']\n",
      "['../data/pubitysport/2023-04-04_11-46-29_UTC.txt', '../data/pubitysport/2021-11-29_20-35-50_UTC.txt', '../data/pubitysport/2018-11-05_09-31-21_UTC.txt', '../data/pubitysport/2020-12-14_19-59-17_UTC.txt', '../data/pubitysport/2021-11-20_15-03-13_UTC.txt', '../data/pubitysport/2023-10-18_19-46-29_UTC.txt', '../data/pubitysport/2023-09-24_13-50-08_UTC.txt', '../data/pubitysport/2018-10-28_15-33-39_UTC.txt']\n",
      "['../data/pubitysport/2020-09-14_14-18-23_UTC.txt', '../data/pubitysport/2019-12-17_16-53-05_UTC.txt', '../data/pubitysport/2021-12-08_21-52-44_UTC.txt', '../data/pubitysport/2023-02-09_09-08-59_UTC.txt', '../data/pubitysport/2023-07-31_13-38-24_UTC.txt', '../data/pubitysport/2021-11-05_14-42-29_UTC.txt', '../data/pubitysport/2021-03-02_22-44-57_UTC.txt', '../data/pubitysport/2020-07-23_19-26-04_UTC.txt']\n",
      "['../data/pubitysport/2019-06-20_10-07-34_UTC.txt', '../data/pubitysport/2019-08-17_18-26-03_UTC.txt', '../data/pubitysport/2022-03-29_20-12-36_UTC.txt', '../data/pubitysport/2022-05-25_10-46-11_UTC.txt', '../data/pubitysport/2019-01-08_17-33-44_UTC.txt', '../data/pubitysport/2020-06-07_13-27-12_UTC.txt', '../data/pubitysport/2023-06-03_15-09-21_UTC.txt', '../data/pubitysport/2019-03-06_23-29-05_UTC.txt']\n",
      "['../data/pubitysport/2020-05-11_18-55-50_UTC.txt', '../data/pubitysport/2018-05-21_19-44-56_UTC.txt', '../data/pubitysport/2018-10-23_19-51-26_UTC.txt', '../data/pubitysport/2021-02-12_17-43-30_UTC.txt', '../data/pubitysport/2019-08-01_15-51-36_UTC.txt', '../data/pubitysport/2021-10-24_05-24-33_UTC.txt', '../data/pubitysport/2018-11-28_19-53-35_UTC.txt', '../data/pubitysport/2019-11-09_19-21-31_UTC.txt']\n",
      "['../data/pubitysport/2020-11-21_14-46-01_UTC.txt', '../data/pubitysport/2019-04-16_13-29-29_UTC.txt', '../data/pubitysport/2020-07-12_20-02-42_UTC.txt', '../data/pubitysport/2023-10-30_09-39-52_UTC.txt', '../data/pubitysport/2022-07-12_19-00-30_UTC.txt', '../data/pubitysport/2020-03-16_10-01-00_UTC.txt', '../data/pubitysport/2022-01-11_11-59-57_UTC.txt', '../data/pubitysport/2020-04-13_19-16-14_UTC.txt']\n",
      "['../data/pubitysport/2020-10-27_13-49-05_UTC.txt', '../data/pubitysport/2020-07-05_11-54-01_UTC.txt', '../data/pubitysport/2023-08-26_20-14-40_UTC.txt', '../data/pubitysport/2020-03-20_13-14-34_UTC.txt', '../data/pubitysport/2019-05-25_18-28-27_UTC.txt', '../data/pubitysport/2019-05-26_13-15-10_UTC.txt', '../data/pubitysport/2023-01-27_11-12-52_UTC.txt', '../data/pubitysport/2020-05-12_09-28-01_UTC.txt']\n",
      "['../data/pubitysport/2019-10-10_12-44-14_UTC.txt', '../data/pubitysport/2020-03-10_16-21-52_UTC.txt', '../data/pubitysport/2023-07-12_14-14-29_UTC.txt', '../data/pubitysport/2023-10-13_21-53-22_UTC.txt', '../data/pubitysport/2021-02-23_15-38-56_UTC.txt', '../data/pubitysport/2023-04-18_18-14-57_UTC.txt', '../data/pubitysport/2023-09-13_12-06-16_UTC.txt', '../data/pubitysport/2021-11-29_10-57-07_UTC.txt']\n",
      "['../data/pubitysport/2023-04-17_19-22-45_UTC.txt', '../data/pubitysport/2018-05-11_17-49-34_UTC.txt', '../data/pubitysport/2019-04-12_13-23-58_UTC.txt', '../data/pubitysport/2019-01-15_16-20-01_UTC.txt', '../data/pubitysport/2018-12-02_18-34-38_UTC.txt', '../data/pubitysport/2019-10-23_10-47-35_UTC.txt', '../data/pubitysport/2023-05-25_22-19-52_UTC.txt', '../data/pubitysport/2021-10-08_06-31-23_UTC.txt']\n",
      "['../data/pubitysport/2023-07-10_14-12-41_UTC.txt', '../data/pubitysport/2018-09-06_12-24-46_UTC.txt', '../data/pubitysport/2020-03-09_10-44-30_UTC.txt', '../data/pubitysport/2018-11-24_12-53-08_UTC.txt', '../data/pubitysport/2023-10-29_20-29-07_UTC.txt', '../data/pubitysport/2021-07-15_19-10-50_UTC.txt', '../data/pubitysport/2018-11-20_19-54-48_UTC.txt', '../data/pubitysport/2023-08-23_14-35-43_UTC.txt']\n",
      "['../data/pubitysport/2020-08-02_18-45-51_UTC.txt', '../data/pubitysport/2022-06-08_14-13-50_UTC.txt', '../data/pubitysport/2022-07-31_12-24-33_UTC.txt', '../data/pubitysport/2020-05-30_17-53-07_UTC.txt', '../data/pubitysport/2018-09-15_07-35-25_UTC.txt', '../data/pubitysport/2023-03-21_16-59-34_UTC.txt', '../data/pubitysport/2019-05-01_21-24-23_UTC.txt', '../data/pubitysport/2021-12-13_12-16-35_UTC.txt']\n",
      "['../data/pubitysport/2020-11-14_19-44-45_UTC.txt', '../data/pubitysport/2023-03-18_22-23-57_UTC.txt', '../data/pubitysport/2021-07-03_11-13-26_UTC.txt', '../data/pubitysport/2022-12-10_17-00-55_UTC.txt', '../data/pubitysport/2019-04-18_21-05-18_UTC.txt', '../data/pubitysport/2019-11-13_21-49-46_UTC.txt', '../data/pubitysport/2023-10-04_10-14-25_UTC.txt', '../data/pubitysport/2020-09-06_18-22-23_UTC.txt']\n",
      "['../data/pubitysport/2020-09-14_15-31-47_UTC.txt', '../data/pubitysport/2023-06-12_15-54-14_UTC.txt', '../data/pubitysport/2021-11-21_10-55-09_UTC.txt', '../data/pubitysport/2021-06-16_14-01-13_UTC.txt', '../data/pubitysport/2020-05-22_13-54-13_UTC.txt', '../data/pubitysport/2023-02-09_13-24-48_UTC.txt', '../data/pubitysport/2022-12-19_10-14-20_UTC.txt', '../data/pubitysport/2021-10-31_21-28-24_UTC.txt']\n",
      "['../data/pubitysport/2021-08-01_18-51-12_UTC.txt', '../data/pubitysport/2023-06-06_18-21-03_UTC.txt', '../data/pubitysport/2021-10-07_18-49-03_UTC.txt', '../data/pubitysport/2021-01-07_17-46-04_UTC.txt', '../data/pubitysport/2020-08-15_15-07-06_UTC.txt', '../data/pubitysport/2022-05-13_18-00-10_UTC.txt', '../data/pubitysport/2023-09-27_20-36-26_UTC.txt', '../data/pubitysport/2021-12-01_11-59-19_UTC.txt']\n",
      "['../data/pubitysport/2023-01-07_13-04-47_UTC.txt', '../data/pubitysport/2023-05-07_08-27-33_UTC.txt', '../data/pubitysport/2019-06-17_09-57-20_UTC.txt', '../data/pubitysport/2023-04-14_16-56-00_UTC.txt', '../data/pubitysport/2019-06-28_15-50-19_UTC.txt', '../data/pubitysport/2020-04-12_08-30-35_UTC.txt', '../data/pubitysport/2020-10-14_20-10-31_UTC.txt', '../data/pubitysport/2020-07-19_21-28-33_UTC.txt']\n",
      "['../data/pubitysport/2021-04-19_17-13-06_UTC.txt', '../data/pubitysport/2020-06-22_14-26-14_UTC.txt', '../data/pubitysport/2020-07-13_15-31-49_UTC.txt', '../data/pubitysport/2019-12-07_21-12-21_UTC.txt', '../data/pubitysport/2020-05-29_18-01-32_UTC.txt', '../data/pubitysport/2019-09-14_18-36-01_UTC.txt', '../data/pubitysport/2022-09-17_11-57-57_UTC.txt', '../data/pubitysport/2023-10-04_21-13-38_UTC.txt']\n",
      "['../data/pubitysport/2019-07-21_18-42-08_UTC.txt', '../data/pubitysport/2019-06-15_19-06-53_UTC.txt', '../data/pubitysport/2020-04-06_13-18-54_UTC.txt', '../data/pubitysport/2020-11-11_21-32-07_UTC.txt', '../data/pubitysport/2020-08-25_15-55-30_UTC.txt', '../data/pubitysport/2020-07-31_08-27-10_UTC.txt', '../data/pubitysport/2023-05-25_17-45-17_UTC.txt', '../data/pubitysport/2022-03-06_16-37-26_UTC.txt']\n",
      "['../data/pubitysport/2020-05-13_13-02-28_UTC.txt', '../data/pubitysport/2019-05-22_14-42-47_UTC.txt', '../data/pubitysport/2023-01-13_09-07-15_UTC.txt', '../data/pubitysport/2020-06-24_19-41-44_UTC.txt', '../data/pubitysport/2020-10-11_15-28-00_UTC.txt', '../data/pubitysport/2019-06-22_21-04-16_UTC.txt', '../data/pubitysport/2021-07-28_14-26-23_UTC.txt', '../data/pubitysport/2020-06-25_09-25-52_UTC.txt']\n",
      "['../data/pubitysport/2023-06-12_10-03-02_UTC.txt', '../data/pubitysport/2018-12-12_17-54-02_UTC.txt', '../data/pubitysport/2020-09-10_13-32-57_UTC.txt', '../data/pubitysport/2021-06-02_15-16-12_UTC.txt', '../data/pubitysport/2023-02-14_23-00-46_UTC.txt', '../data/pubitysport/2019-06-16_09-55-52_UTC.txt', '../data/pubitysport/2019-12-16_12-50-13_UTC.txt', '../data/pubitysport/2019-07-20_13-34-32_UTC.txt']\n",
      "['../data/pubitysport/2021-11-28_19-44-14_UTC.txt', '../data/pubitysport/2019-08-03_16-55-18_UTC.txt', '../data/pubitysport/2019-02-28_10-30-28_UTC.txt', '../data/pubitysport/2023-07-17_09-32-30_UTC.txt', '../data/pubitysport/2022-10-26_15-25-22_UTC.txt', '../data/pubitysport/2021-02-21_13-35-18_UTC.txt', '../data/pubitysport/2019-05-29_21-47-49_UTC.txt', '../data/pubitysport/2019-06-22_21-03-40_UTC.txt']\n",
      "['../data/pubitysport/2021-07-01_11-00-32_UTC.txt', '../data/pubitysport/2020-10-19_09-43-22_UTC.txt', '../data/pubitysport/2019-07-03_18-15-30_UTC.txt', '../data/pubitysport/2023-10-30_20-51-23_UTC.txt', '../data/pubitysport/2023-06-22_21-55-25_UTC.txt', '../data/pubitysport/2022-12-03_14-15-12_UTC.txt', '../data/pubitysport/2021-12-18_07-24-42_UTC.txt', '../data/pubitysport/2023-05-03_07-37-15_UTC.txt']\n",
      "['../data/pubitysport/2019-12-08_12-32-19_UTC.txt', '../data/pubitysport/2019-03-06_22-26-47_UTC.txt', '../data/pubitysport/2020-10-21_11-02-59_UTC.txt', '../data/pubitysport/2019-03-22_22-13-46_UTC.txt', '../data/pubitysport/2020-06-23_18-51-29_UTC.txt', '../data/pubitysport/2020-12-08_15-22-22_UTC.txt', '../data/pubitysport/2021-10-27_13-10-11_UTC.txt', '../data/pubitysport/2023-07-20_07-30-00_UTC.txt']\n",
      "['../data/pubitysport/2021-09-27_11-54-58_UTC.txt', '../data/pubitysport/2019-01-28_14-50-14_UTC.txt', '../data/pubitysport/2023-04-24_18-30-21_UTC.txt', '../data/pubitysport/2022-09-13_18-36-50_UTC.txt', '../data/pubitysport/2020-03-05_15-08-28_UTC.txt', '../data/pubitysport/2020-08-12_21-00-46_UTC.txt', '../data/pubitysport/2018-10-29_11-21-09_UTC.txt', '../data/pubitysport/2023-06-29_11-06-14_UTC.txt']\n",
      "['../data/pubitysport/2020-03-15_16-43-49_UTC.txt', '../data/pubitysport/2020-10-25_17-07-36_UTC.txt', '../data/pubitysport/2020-04-09_16-05-56_UTC.txt', '../data/pubitysport/2023-09-23_16-23-11_UTC.txt', '../data/pubitysport/2018-12-18_09-55-50_UTC.txt', '../data/pubitysport/2018-12-12_21-22-32_UTC.txt', '../data/pubitysport/2020-07-11_13-26-25_UTC.txt', '../data/pubitysport/2018-11-08_08-44-26_UTC.txt']\n",
      "['../data/pubitysport/2018-11-15_13-14-09_UTC.txt', '../data/pubitysport/2020-02-28_15-16-37_UTC.txt', '../data/pubitysport/2022-07-10_13-41-34_UTC.txt', '../data/pubitysport/2020-08-27_20-37-43_UTC.txt', '../data/pubitysport/2019-02-10_17-22-15_UTC.txt', '../data/pubitysport/2023-09-21_14-56-42_UTC.txt', '../data/pubitysport/2019-01-20_17-26-09_UTC.txt', '../data/pubitysport/2019-08-22_13-06-19_UTC.txt']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenizer(batch_contents, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 11\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mbertweet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m [o\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state]  \u001b[38;5;66;03m# This contains the embeddings for each token in the input\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     all_embeddings\u001b[38;5;241m.\u001b[39mextend(embeddings)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp-project/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp-project/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp-project/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:835\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    826\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    828\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    829\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    830\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    833\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    834\u001b[0m )\n\u001b[0;32m--> 835\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    847\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    848\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp-project/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp-project/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp-project/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:524\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    513\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    514\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    515\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    521\u001b[0m         output_attentions,\n\u001b[1;32m    522\u001b[0m     )\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 524\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    534\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp-project/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp-project/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp-project/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:413\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    403\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    410\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    412\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 413\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp-project/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp-project/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp-project/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:340\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    332\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    338\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    339\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 340\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    350\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp-project/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp-project/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp-project/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:220\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m     key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey(hidden_states))\n\u001b[0;32m--> 220\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    222\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n\u001b[1;32m    224\u001b[0m use_cache \u001b[38;5;241m=\u001b[39m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp-project/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp-project/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp-project/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "591db88c-9599-4137-abd1-150522bd33b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62, 768)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b36b0d77-9b59-46c2-8396-7cbeb6dc4ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_vectorizer(\n",
    "    username: str,\n",
    "    root: str = \"../data\",\n",
    "    how: Union[str, \"tfidf\", \"bow\", \"bert\"] = \"tfidf\",\n",
    "    ngram_range: Tuple[int, int] = (1, 1),\n",
    "    fit_before: Union[str, datetime] = datetime.today()\n",
    ") -> Tuple[np.ndarray, TfidfVectorizer]:\n",
    "    \"\"\"\n",
    "    Vectorizes the content of a given profile. Assumes the download\n",
    "    has already been done, and the directory is full of posts. Directory\n",
    "    is expected to contain a folder named after the profile, which contains\n",
    "    the text files and json metadata files for each post.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    username : str\n",
    "        Username of the profile to vectorize\n",
    "    root : str\n",
    "        Path to the folder where the posts are stored\n",
    "    how : str\n",
    "        How to vectorize the content. Can be either \"tfidf\", \"bow\" (bag of words), or \"bert\"\n",
    "    return_vectorizer : bool\n",
    "        Whether to return the vectorizer object or not\n",
    "    ngram_range : Tuple[int, int]\n",
    "        Range of ngrams to use for tfidf or count vectorization\n",
    "    fit_before : Union[str, datetime]\n",
    "        Date to fit the vectorizer before. Can be either a datetime object\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[np.ndarray, Union[TfidfVectorizer, None]]\n",
    "        Tuple containing the vectors and the vectorizer object if\n",
    "        return_vectorizer is True, None otherwise\n",
    "    \"\"\"\n",
    "    if how in [\"tfidf\", \"bow\"]:\n",
    "        profile_path = os.path.join(root, username)\n",
    "        text_files = [\n",
    "            os.path.join(profile_path, file)\n",
    "            for file in os.listdir(profile_path)\n",
    "            if file.endswith(\".txt\")\n",
    "            and datetime.fromtimestamp(os.path.getmtime(os.path.join(profile_path, file))) < fit_before\n",
    "        ]\n",
    "        vec_class = TfidfVectorizer if how == \"tfidf\" else CountVectorizer\n",
    "        vectorizer = vec_class(input=\"filename\", strip_accents=\"unicode\", ngram_range=ngram_range)\n",
    "        vectors = vectorizer.fit_transform(text_files)\n",
    "\n",
    "    elif how == \"bert\":\n",
    "        raise NotImplementedError(\"bert vectorization not implemented yet\")\n",
    "    else:\n",
    "        raise NotImplementedError(\"how must be either tfidf or bert\")\n",
    "\n",
    "    # Necessary to work with raw text inputs after training on documents\n",
    "    vectorizer.input = 'content'\n",
    "    return vectors, vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5361e9d1-e00c-455d-be46-13c4be698cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trends(hl='en-US', geo=\"US\", tz=360, count=20, date: Union[datetime, str] = datetime.today()) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch Google Trends realtime data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    hl : str\n",
    "        Language\n",
    "    geo : str\n",
    "        Country\n",
    "    tz : int\n",
    "        Timezone\n",
    "    count : int\n",
    "        Number of results\n",
    "    date : Union[datetime, str]\n",
    "        Date to fetch data from. Can be either a datetime object or a string\n",
    "        in the format YYYY-MM-DD (fuzzy parsing is enabled, but not recommended)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing the results\n",
    "    \"\"\"\n",
    "    if isinstance(date, str):\n",
    "        date = dateparser.parse(date, fuzzy=True)\n",
    "    \n",
    "    response = requests.get(\n",
    "        \"https://trends.google.com/trends/api/dailytrends\",\n",
    "        params={\n",
    "            \"hl\": hl,\n",
    "            \"tz\": tz,\n",
    "            \"ed\": date.strftime(\"%Y%m%d\"),\n",
    "            \"geo\": geo,\n",
    "            \"ns\": count,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    response.raise_for_status()\n",
    "    if response.status_code != 204:\n",
    "        data = response.text.split(\")]}\\',\\n\")[1]\n",
    "        data = json.loads(data)[\"default\"][\"trendingSearchesDays\"][0][\"trendingSearches\"]\n",
    "\n",
    "    dfs = pd.concat([pd.DataFrame(trend['articles']) for trend in data], ignore_index=True)\n",
    "           \n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd8d4893-fdd5-4dd4-a99a-10e73386d24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article(url: str):\n",
    "    \"\"\"\n",
    "    Obtains the likely text from an article based on the newspaper library\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url: str\n",
    "        URL of the article to fetch\n",
    "\n",
    "    Returns \n",
    "    -------\n",
    "    str\n",
    "        Most likely article text \n",
    "    \"\"\"\n",
    "    \n",
    "    text = \"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4644.45 Safari/537.36\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        article = Article(url)\n",
    "        article.download(input_html=response.text)\n",
    "        article.parse()\n",
    "        text = article.text            \n",
    "\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        pass\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8424a643-cdc5-4488-af19-b46688e204d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_metadata_json(fp: os.PathLike):\n",
    "    with open(fp, 'r') as f:\n",
    "        metadata = json.loads(f.read())[\"node\"]\n",
    "    \n",
    "        dt = datetime.strptime(os.path.basename(fp), \"%Y-%m-%d_%H-%M-%S_UTC.json\")\n",
    "    \n",
    "        clean_metadata = {\n",
    "            \"dt\": dt,\n",
    "            \"likes\": metadata[\"edge_media_preview_like\"][\"count\"],\n",
    "            \"comments\": metadata[\"edge_media_to_comment\"][\"count\"],\n",
    "            \"caption\": metadata[\"edge_media_to_caption\"][\"edges\"][0][\"node\"][\"text\"] if metadata[\"edge_media_to_caption\"][\"edges\"] else \"\",\n",
    "            \"comments_disabled\": metadata[\"comments_disabled\"],\n",
    "            \"is_video\": metadata[\"is_video\"],\n",
    "            \"tagged_users\": metadata[\"edge_media_to_tagged_user\"]\n",
    "            \n",
    "        }\n",
    "\n",
    "        return clean_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae9a95ff-121b-4b20-8ffd-9f91bbc00e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posts(\n",
    "        username: str,      \n",
    "        root: os.PathLike = \"../data\",\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads all the posts of a given profile and returns a DataFrame\n",
    "    with the relevant information\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    username : str\n",
    "        Username of the profile to read\n",
    "    root : os.PathLike\n",
    "        Path to the folder where the posts are stored\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing the relevant metadata\n",
    "    \"\"\"\n",
    "    profile_path = os.path.join(root, username)\n",
    "    json_files = [\n",
    "        os.path.join(profile_path, file)\n",
    "        for file in os.listdir(profile_path)\n",
    "        if file.endswith(\"UTC.json\")\n",
    "    ]\n",
    "    metadata = [read_metadata_json(file) for file in json_files]\n",
    "    df = pd.DataFrame(metadata)\n",
    "    df.set_index(\"dt\", inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cde1c24-02de-4592-a5e2-84802a34dc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trends = get_trends()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7daa4ba7-a43b-4cae-8202-f06e7df558d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>timeAgo</th>\n",
       "      <th>source</th>\n",
       "      <th>image</th>\n",
       "      <th>url</th>\n",
       "      <th>snippet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>**!!((S T R E A M))* Pittsburgh Steelers vs Te...</td>\n",
       "      <td>15h ago</td>\n",
       "      <td>Mageecourier-countynews</td>\n",
       "      <td>{'newsUrl': 'https://www.simpsoncounty.ms/node...</td>\n",
       "      <td>https://www.simpsoncounty.ms/node/286444</td>\n",
       "      <td>Category : Tennessee Titans vs. Pittsburgh Ste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Steelers shut door in fourth quarter, continue...</td>\n",
       "      <td>10h ago</td>\n",
       "      <td>ESPN</td>\n",
       "      <td>{'newsUrl': 'https://www.espn.com/nfl/story/_/...</td>\n",
       "      <td>https://www.espn.com/nfl/story/_/id/38804709/s...</td>\n",
       "      <td>Linebacker Kwon Alexander thwarts the Titans&amp;#...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Titans Fall 20-16 to Steelers, Drop to 3-5</td>\n",
       "      <td>9h ago</td>\n",
       "      <td>Tennessee Titans</td>\n",
       "      <td>{'newsUrl': 'https://www.tennesseetitans.com/n...</td>\n",
       "      <td>https://www.tennesseetitans.com/news/titans-fa...</td>\n",
       "      <td>The Steelers then took the lead 20-16 on a thr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Titans vs. Steelers highlights: Pittsburgh win...</td>\n",
       "      <td>7h ago</td>\n",
       "      <td>FOXSports.com</td>\n",
       "      <td>{'newsUrl': 'https://www.foxsports.com/stories...</td>\n",
       "      <td>https://www.foxsports.com/stories/nfl/titans-v...</td>\n",
       "      <td>Week 9 of the NFL season begins Thursday with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Steelers and Kenny Pickett exorcise some offen...</td>\n",
       "      <td>3h ago</td>\n",
       "      <td>Yahoo Sports</td>\n",
       "      <td>{'newsUrl': 'https://sports.yahoo.com/steelers...</td>\n",
       "      <td>https://sports.yahoo.com/steelers-and-kenny-pi...</td>\n",
       "      <td>The calls for Matt Canada&amp;#39;s job won&amp;#39;t ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>Hezbollah, Israel exchange fire as violence sp...</td>\n",
       "      <td>19h ago</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>{'newsUrl': 'https://www.reuters.com/world/mid...</td>\n",
       "      <td>https://www.reuters.com/world/middle-east/leba...</td>\n",
       "      <td>Lebanon&amp;#39;s Hezbollah said on Thursday it mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>Lebanon-Israel border fighting picks up before...</td>\n",
       "      <td>18h ago</td>\n",
       "      <td>Aljazeera.com</td>\n",
       "      <td>{'newsUrl': 'https://www.aljazeera.com/news/20...</td>\n",
       "      <td>https://www.aljazeera.com/news/2023/11/2/leban...</td>\n",
       "      <td>Hezbollah chief Hassan Nasrallah is set to spe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>Hezbollah&amp;#39;s Nasrallah to break silence as ...</td>\n",
       "      <td>4h ago</td>\n",
       "      <td>The Times of Israel</td>\n",
       "      <td>{'newsUrl': 'https://www.timesofisrael.com/hez...</td>\n",
       "      <td>https://www.timesofisrael.com/hezbollahs-nasra...</td>\n",
       "      <td>Lebanon&amp;#39;s Hezbollah chief Hassan Nasrallah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Israel-Hamas war live: Hezbollah chief Hassan ...</td>\n",
       "      <td>4h ago</td>\n",
       "      <td>Aljazeera.com</td>\n",
       "      <td>{'newsUrl': 'https://www.aljazeera.com/news/li...</td>\n",
       "      <td>https://www.aljazeera.com/news/liveblog/2023/1...</td>\n",
       "      <td>Israeli authorities send back thousands of Pal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Hezbollah leader to break his silence with pub...</td>\n",
       "      <td>6h ago</td>\n",
       "      <td>CNN</td>\n",
       "      <td>{'newsUrl': 'https://www.cnn.com/videos/world/...</td>\n",
       "      <td>https://www.cnn.com/videos/world/2023/11/03/ex...</td>\n",
       "      <td>Hassan Nasrallah is expected to make his first...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  timeAgo  \\\n",
       "0    **!!((S T R E A M))* Pittsburgh Steelers vs Te...  15h ago   \n",
       "1    Steelers shut door in fourth quarter, continue...  10h ago   \n",
       "2           Titans Fall 20-16 to Steelers, Drop to 3-5   9h ago   \n",
       "3    Titans vs. Steelers highlights: Pittsburgh win...   7h ago   \n",
       "4    Steelers and Kenny Pickett exorcise some offen...   3h ago   \n",
       "..                                                 ...      ...   \n",
       "126  Hezbollah, Israel exchange fire as violence sp...  19h ago   \n",
       "127  Lebanon-Israel border fighting picks up before...  18h ago   \n",
       "128  Hezbollah&#39;s Nasrallah to break silence as ...   4h ago   \n",
       "129  Israel-Hamas war live: Hezbollah chief Hassan ...   4h ago   \n",
       "130  Hezbollah leader to break his silence with pub...   6h ago   \n",
       "\n",
       "                      source  \\\n",
       "0    Mageecourier-countynews   \n",
       "1                       ESPN   \n",
       "2           Tennessee Titans   \n",
       "3              FOXSports.com   \n",
       "4               Yahoo Sports   \n",
       "..                       ...   \n",
       "126                  Reuters   \n",
       "127            Aljazeera.com   \n",
       "128      The Times of Israel   \n",
       "129            Aljazeera.com   \n",
       "130                      CNN   \n",
       "\n",
       "                                                 image  \\\n",
       "0    {'newsUrl': 'https://www.simpsoncounty.ms/node...   \n",
       "1    {'newsUrl': 'https://www.espn.com/nfl/story/_/...   \n",
       "2    {'newsUrl': 'https://www.tennesseetitans.com/n...   \n",
       "3    {'newsUrl': 'https://www.foxsports.com/stories...   \n",
       "4    {'newsUrl': 'https://sports.yahoo.com/steelers...   \n",
       "..                                                 ...   \n",
       "126  {'newsUrl': 'https://www.reuters.com/world/mid...   \n",
       "127  {'newsUrl': 'https://www.aljazeera.com/news/20...   \n",
       "128  {'newsUrl': 'https://www.timesofisrael.com/hez...   \n",
       "129  {'newsUrl': 'https://www.aljazeera.com/news/li...   \n",
       "130  {'newsUrl': 'https://www.cnn.com/videos/world/...   \n",
       "\n",
       "                                                   url  \\\n",
       "0             https://www.simpsoncounty.ms/node/286444   \n",
       "1    https://www.espn.com/nfl/story/_/id/38804709/s...   \n",
       "2    https://www.tennesseetitans.com/news/titans-fa...   \n",
       "3    https://www.foxsports.com/stories/nfl/titans-v...   \n",
       "4    https://sports.yahoo.com/steelers-and-kenny-pi...   \n",
       "..                                                 ...   \n",
       "126  https://www.reuters.com/world/middle-east/leba...   \n",
       "127  https://www.aljazeera.com/news/2023/11/2/leban...   \n",
       "128  https://www.timesofisrael.com/hezbollahs-nasra...   \n",
       "129  https://www.aljazeera.com/news/liveblog/2023/1...   \n",
       "130  https://www.cnn.com/videos/world/2023/11/03/ex...   \n",
       "\n",
       "                                               snippet  \n",
       "0    Category : Tennessee Titans vs. Pittsburgh Ste...  \n",
       "1    Linebacker Kwon Alexander thwarts the Titans&#...  \n",
       "2    The Steelers then took the lead 20-16 on a thr...  \n",
       "3    Week 9 of the NFL season begins Thursday with ...  \n",
       "4    The calls for Matt Canada&#39;s job won&#39;t ...  \n",
       "..                                                 ...  \n",
       "126  Lebanon&#39;s Hezbollah said on Thursday it mo...  \n",
       "127  Hezbollah chief Hassan Nasrallah is set to spe...  \n",
       "128  Lebanon&#39;s Hezbollah chief Hassan Nasrallah...  \n",
       "129  Israeli authorities send back thousands of Pal...  \n",
       "130  Hassan Nasrallah is expected to make his first...  \n",
       "\n",
       "[131 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f5e6aa79-4682-4348-a26f-7f9364b277c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnsutor/anaconda3/envs/nlp-project/lib/python3.11/site-packages/dateutil/parser/_parser.py:1207: UnknownTimezoneWarning: tzname CDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "/Users/johnsutor/anaconda3/envs/nlp-project/lib/python3.11/site-packages/dateutil/parser/_parser.py:1207: UnknownTimezoneWarning: tzname MDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n"
     ]
    }
   ],
   "source": [
    "trends[\"text\"] = trends[\"url\"].apply(get_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "36f06ea6-ea5b-455c-922c-1560eb8d82a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors, vectorizer = fit_vectorizer(\"pubitysport\", how=\"tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "28f956e9-d197-4c6d-a46a-ed5fbd9a2f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = get_posts(\"pubitysport\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e7a76e33-e125-4c73-a490-f094645e4913",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts['embedding'] = posts['caption'].apply(lambda x: vectorizer.transform([x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b91eacd-a310-4d3d-99ed-d9df8201bc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ema_sparse_array(data, alpha):\n",
    "    ema = []\n",
    "    ema_value = None\n",
    "\n",
    "    for value in data:\n",
    "        if ema_value is None:\n",
    "            ema_value = value\n",
    "        else:\n",
    "            ema_value = (value - ema_value) * alpha + ema_value\n",
    "        ema.append(ema_value)\n",
    "\n",
    "    return ema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1945e416-7a5c-45c6-9caa-d036353b4c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts['embedding_ema'] = ema_sparse_array(posts['embedding'], 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4e5e6489-3638-482f-9adf-5d80fb0b3280",
   "metadata": {},
   "outputs": [],
   "source": [
    "trends['embedding'] = trends['text'].apply(lambda x: vectorizer.transform([x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c27ce9f7-0b1a-49ec-963a-70719f99c137",
   "metadata": {},
   "outputs": [],
   "source": [
    "trends['score'] = trends['embedding'].apply(lambda x: cosine_similarity(x, posts['embedding_ema'].iloc[-1])[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b08692e8-f30b-4e40-a41a-213ccc632dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>timeAgo</th>\n",
       "      <th>source</th>\n",
       "      <th>image</th>\n",
       "      <th>url</th>\n",
       "      <th>snippet</th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>World Series 2023: Live updates from Rangers p...</td>\n",
       "      <td>1h ago</td>\n",
       "      <td>The Dallas Morning News</td>\n",
       "      <td>{'newsUrl': 'https://www.dallasnews.com/news/2...</td>\n",
       "      <td>https://www.dallasnews.com/news/2023/11/03/tex...</td>\n",
       "      <td>A team of Dallas Morning News staffers will be...</td>\n",
       "      <td>The Texas Rangers have returned to their home ...</td>\n",
       "      <td>(0, 15926)\\t0.008895202478043075\\n  (0, 1591...</td>\n",
       "      <td>0.128359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Yes, they&amp;#39;ve already picked the Rockefelle...</td>\n",
       "      <td>23h ago</td>\n",
       "      <td>GPB</td>\n",
       "      <td>{'newsUrl': 'https://www.gpb.org/news/2023/11/...</td>\n",
       "      <td>https://www.gpb.org/news/2023/11/02/yes-theyve...</td>\n",
       "      <td>The Rockefeller Center Christmas tree is comin...</td>\n",
       "      <td>The Rockefeller Center Christmas tree is comin...</td>\n",
       "      <td>(0, 15935)\\t0.022177823251572158\\n  (0, 1592...</td>\n",
       "      <td>0.107895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>City of Arlington to Host Texas Rangers World ...</td>\n",
       "      <td>16h ago</td>\n",
       "      <td>City of Arlington</td>\n",
       "      <td>{'newsUrl': 'https://www.arlingtontx.gov/news/...</td>\n",
       "      <td>https://www.arlingtontx.gov/news/my_arlington_...</td>\n",
       "      <td>Get the latest news delivered straight to your...</td>\n",
       "      <td>City of Arlington to Host Texas Rangers World ...</td>\n",
       "      <td>(0, 15898)\\t0.012901552571298495\\n  (0, 1589...</td>\n",
       "      <td>0.106151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Earth&amp;#39;s new future is here in Kingdom of t...</td>\n",
       "      <td>23h ago</td>\n",
       "      <td>EW.com</td>\n",
       "      <td>{'newsUrl': 'https://ew.com/movies/kingdom-of-...</td>\n",
       "      <td>https://ew.com/movies/kingdom-of-the-planet-of...</td>\n",
       "      <td>The war may be over, but the planet of the ape...</td>\n",
       "      <td>The war may be over, but the planet of the ape...</td>\n",
       "      <td>(0, 15928)\\t0.03313274439783441\\n  (0, 15926...</td>\n",
       "      <td>0.104056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Tommy Tuberville&amp;#39;s Military Blockade Is Th...</td>\n",
       "      <td>1h ago</td>\n",
       "      <td>Rolling Stone</td>\n",
       "      <td>{'newsUrl': 'https://www.rollingstone.com/poli...</td>\n",
       "      <td>https://www.rollingstone.com/politics/politics...</td>\n",
       "      <td>Is Tommy Tuberville the Most Ignorant Man in D...</td>\n",
       "      <td>Tommy Tubervilleâ€™s Republican colleagues had f...</td>\n",
       "      <td>(0, 15988)\\t0.0058343504296261344\\n  (0, 159...</td>\n",
       "      <td>0.102152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Fortnite OG: Drop Back into Battle Royale Chap...</td>\n",
       "      <td>7h ago</td>\n",
       "      <td>Epic Games</td>\n",
       "      <td>{'newsUrl': 'https://www.fortnite.com/news/for...</td>\n",
       "      <td>https://www.fortnite.com/news/fortnite-og-drop...</td>\n",
       "      <td>Drop in the original Island once again in Fort...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Run It Back with the OG Pass and OG Shop in Fo...</td>\n",
       "      <td>7h ago</td>\n",
       "      <td>Epic Games</td>\n",
       "      <td>{'newsUrl': 'https://www.fortnite.com/news/run...</td>\n",
       "      <td>https://www.fortnite.com/news/run-it-back-with...</td>\n",
       "      <td>The time-traveling turbo Season Fortnite OG me...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Duke Football Secures Bowl Eligibility With Wa...</td>\n",
       "      <td>3h ago</td>\n",
       "      <td>CalBearsMaven</td>\n",
       "      <td>{'newsUrl': 'https://www.si.com/college/duke/f...</td>\n",
       "      <td>https://www.si.com/college/duke/football/duke-...</td>\n",
       "      <td>Following Thursday night&amp;#39;s battle-winning ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>What to know about the Texas Rangers&amp;#39; Worl...</td>\n",
       "      <td>21h ago</td>\n",
       "      <td>Axios</td>\n",
       "      <td>{'newsUrl': 'https://www.axios.com/local/dalla...</td>\n",
       "      <td>https://www.axios.com/local/dallas/2023/11/02/...</td>\n",
       "      <td>Rangers fans will finally get to experience so...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Steelers Fear Worst With Cole Holcomb Injury</td>\n",
       "      <td>4h ago</td>\n",
       "      <td>Sports Illustrated</td>\n",
       "      <td>{'newsUrl': 'https://www.si.com/nfl/steelers/n...</td>\n",
       "      <td>https://www.si.com/nfl/steelers/news/pittsburg...</td>\n",
       "      <td>Mike Tomlin offered &amp;quot;thoughts and prayers...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  timeAgo  \\\n",
       "57  World Series 2023: Live updates from Rangers p...   1h ago   \n",
       "95  Yes, they&#39;ve already picked the Rockefelle...  23h ago   \n",
       "56  City of Arlington to Host Texas Rangers World ...  16h ago   \n",
       "52  Earth&#39;s new future is here in Kingdom of t...  23h ago   \n",
       "86  Tommy Tuberville&#39;s Military Blockade Is Th...   1h ago   \n",
       "..                                                ...      ...   \n",
       "24  Fortnite OG: Drop Back into Battle Royale Chap...   7h ago   \n",
       "25  Run It Back with the OG Pass and OG Shop in Fo...   7h ago   \n",
       "72  Duke Football Secures Bowl Eligibility With Wa...   3h ago   \n",
       "58  What to know about the Texas Rangers&#39; Worl...  21h ago   \n",
       "6        Steelers Fear Worst With Cole Holcomb Injury   4h ago   \n",
       "\n",
       "                     source  \\\n",
       "57  The Dallas Morning News   \n",
       "95                      GPB   \n",
       "56        City of Arlington   \n",
       "52                   EW.com   \n",
       "86            Rolling Stone   \n",
       "..                      ...   \n",
       "24               Epic Games   \n",
       "25               Epic Games   \n",
       "72            CalBearsMaven   \n",
       "58                    Axios   \n",
       "6        Sports Illustrated   \n",
       "\n",
       "                                                image  \\\n",
       "57  {'newsUrl': 'https://www.dallasnews.com/news/2...   \n",
       "95  {'newsUrl': 'https://www.gpb.org/news/2023/11/...   \n",
       "56  {'newsUrl': 'https://www.arlingtontx.gov/news/...   \n",
       "52  {'newsUrl': 'https://ew.com/movies/kingdom-of-...   \n",
       "86  {'newsUrl': 'https://www.rollingstone.com/poli...   \n",
       "..                                                ...   \n",
       "24  {'newsUrl': 'https://www.fortnite.com/news/for...   \n",
       "25  {'newsUrl': 'https://www.fortnite.com/news/run...   \n",
       "72  {'newsUrl': 'https://www.si.com/college/duke/f...   \n",
       "58  {'newsUrl': 'https://www.axios.com/local/dalla...   \n",
       "6   {'newsUrl': 'https://www.si.com/nfl/steelers/n...   \n",
       "\n",
       "                                                  url  \\\n",
       "57  https://www.dallasnews.com/news/2023/11/03/tex...   \n",
       "95  https://www.gpb.org/news/2023/11/02/yes-theyve...   \n",
       "56  https://www.arlingtontx.gov/news/my_arlington_...   \n",
       "52  https://ew.com/movies/kingdom-of-the-planet-of...   \n",
       "86  https://www.rollingstone.com/politics/politics...   \n",
       "..                                                ...   \n",
       "24  https://www.fortnite.com/news/fortnite-og-drop...   \n",
       "25  https://www.fortnite.com/news/run-it-back-with...   \n",
       "72  https://www.si.com/college/duke/football/duke-...   \n",
       "58  https://www.axios.com/local/dallas/2023/11/02/...   \n",
       "6   https://www.si.com/nfl/steelers/news/pittsburg...   \n",
       "\n",
       "                                              snippet  \\\n",
       "57  A team of Dallas Morning News staffers will be...   \n",
       "95  The Rockefeller Center Christmas tree is comin...   \n",
       "56  Get the latest news delivered straight to your...   \n",
       "52  The war may be over, but the planet of the ape...   \n",
       "86  Is Tommy Tuberville the Most Ignorant Man in D...   \n",
       "..                                                ...   \n",
       "24  Drop in the original Island once again in Fort...   \n",
       "25  The time-traveling turbo Season Fortnite OG me...   \n",
       "72  Following Thursday night&#39;s battle-winning ...   \n",
       "58  Rangers fans will finally get to experience so...   \n",
       "6   Mike Tomlin offered &quot;thoughts and prayers...   \n",
       "\n",
       "                                                 text  \\\n",
       "57  The Texas Rangers have returned to their home ...   \n",
       "95  The Rockefeller Center Christmas tree is comin...   \n",
       "56  City of Arlington to Host Texas Rangers World ...   \n",
       "52  The war may be over, but the planet of the ape...   \n",
       "86  Tommy Tubervilleâ€™s Republican colleagues had f...   \n",
       "..                                                ...   \n",
       "24                                                      \n",
       "25                                                      \n",
       "72                                                      \n",
       "58                                                      \n",
       "6                                                       \n",
       "\n",
       "                                            embedding     score  \n",
       "57    (0, 15926)\\t0.008895202478043075\\n  (0, 1591...  0.128359  \n",
       "95    (0, 15935)\\t0.022177823251572158\\n  (0, 1592...  0.107895  \n",
       "56    (0, 15898)\\t0.012901552571298495\\n  (0, 1589...  0.106151  \n",
       "52    (0, 15928)\\t0.03313274439783441\\n  (0, 15926...  0.104056  \n",
       "86    (0, 15988)\\t0.0058343504296261344\\n  (0, 159...  0.102152  \n",
       "..                                                ...       ...  \n",
       "24                                                     0.000000  \n",
       "25                                                     0.000000  \n",
       "72                                                     0.000000  \n",
       "58                                                     0.000000  \n",
       "6                                                      0.000000  \n",
       "\n",
       "[131 rows x 9 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trends.sort_values(by='score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f9683b-4c5b-47d7-9506-4f1b66629328",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
