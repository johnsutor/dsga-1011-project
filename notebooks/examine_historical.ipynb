{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407cf7e5-190d-47f8-ae73-ef39c6a85219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "import instaloader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "from dateutil import parser as dateparser\n",
    "from newspaper import Article\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import nltk\n",
    "import time\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity, manhattan_distances, euclidean_distances\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "def download_profile(\n",
    "    usernames: Union[str, List[str]], root: os.PathLike = \"../data\", **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Downloads all posts of a given publicly-accessible profile.\n",
    "    Does not download images or videos, only metadata\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    usernames : str, List[str]\n",
    "        Username(s) of the profile to download\n",
    "    root : str\n",
    "        Path to the folder where the posts will be stored\n",
    "    \"\"\"\n",
    "\n",
    "    loader = instaloader.Instaloader(\n",
    "        dirname_pattern=os.path.join(root, \"{profile}\"),\n",
    "        download_pictures=False,\n",
    "        download_videos=False,\n",
    "        download_video_thumbnails=False,\n",
    "        download_geotags=False,\n",
    "        download_comments=False,\n",
    "        save_metadata=True,\n",
    "        compress_json=False,\n",
    "    )\n",
    "\n",
    "    if isinstance(usernames, str):\n",
    "        usernames = [usernames]\n",
    "\n",
    "    profiles = [\n",
    "        instaloader.Profile.from_username(loader.context, username)\n",
    "        for username in usernames\n",
    "    ]\n",
    "\n",
    "    latest_stamps = instaloader.LatestStamps(os.path.join(root, \"latest_timestamp.txt\"))\n",
    "\n",
    "    loader.download_profiles(\n",
    "        profiles,\n",
    "        fast_update=True,\n",
    "        profile_pic=False,\n",
    "        igtv=False,\n",
    "        latest_stamps=latest_stamps,\n",
    "        stories=False,\n",
    "        highlights=False,\n",
    "        tagged=False,\n",
    "    )\n",
    "\n",
    "\n",
    "class Vectorizer:\n",
    "    \"\"\"\n",
    "    A class for vectorizing text inputs\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, how: str = \"tfidf\", ngram_range: Tuple[int, int] = (1, 1)):\n",
    "        \"\"\"\n",
    "        Initializes the vectorizer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        how : str\n",
    "            How to vectorize the content. Can be either \"tfidf\", \"bow\" (bag of words), or \"bert\"\n",
    "        ngram_range : Tuple[int, int]\n",
    "            Range of ngrams to use for tfidf or count vectorization\n",
    "        \"\"\"\n",
    "        self.how = how\n",
    "        self.ngram_range = ngram_range\n",
    "        if self.how in [\"tfidf\", \"bow\"]:\n",
    "            self.vectorizer = (\n",
    "                TfidfVectorizer if self.how == \"tfidf\" else CountVectorizer\n",
    "            )(\n",
    "                input=\"filename\",\n",
    "                strip_accents=\"unicode\",\n",
    "                ngram_range=self.ngram_range,\n",
    "            )\n",
    "        elif self.how == \"bert\":\n",
    "            self.vectorizer = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                \"vinai/bertweet-base\", use_fast=False\n",
    "            )\n",
    "        elif self.how == \"roberta\":\n",
    "            self.vectorizer = SentenceTransformer(\"all-distilroberta-v1\")\n",
    "        else:\n",
    "            raise NotImplementedError(\"how must be either tfidf, bow or bert\")\n",
    "        self.trained = False\n",
    "\n",
    "    def fit_transform(\n",
    "        self, text_files: List[os.PathLike], batch_size: int = 8\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Fits the vectorizer to the given text files, and returns the vectors\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text_files : List[os.PathLike]\n",
    "            List of paths to the text files to fit the vectorizer to\n",
    "        batch_size : int\n",
    "            Batch size for bert vectorization\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Array containing the vectors\n",
    "        \"\"\"\n",
    "        if self.how in [\"tfidf\", \"bow\"]:\n",
    "            vectors = self.vectorizer.fit_transform(text_files).toarray()\n",
    "            self.vectorizer.input = \"content\"\n",
    "        elif self.how == \"bert\":\n",
    "            all_embeddings = []\n",
    "\n",
    "            for i in tqdm(range(0, len(text_files), 8), desc=\"Bert vectorization\"):\n",
    "                batch_contents = [\n",
    "                    open(file_path, \"r\", encoding=\"utf-8\").read()\n",
    "                    for file_path in text_files[i : i + 8]\n",
    "                    if os.path.exists(file_path)\n",
    "                ]\n",
    "\n",
    "                tokens = self.tokenizer(\n",
    "                    batch_contents, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "                )\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.vectorizer(**tokens)\n",
    "                    embeddings = [\n",
    "                        o.numpy() for o in outputs.last_hidden_state\n",
    "                    ]  # This contains the embeddings for each token in the input\n",
    "                    all_embeddings.extend(embeddings)\n",
    "\n",
    "            vectors = np.array(all_embeddings)\n",
    "        else:\n",
    "            raise NotImplementedError(\"how must be either tfidf, bow or bert\")\n",
    "\n",
    "        self.trained = True\n",
    "        return vectors\n",
    "\n",
    "    def fit(self, text_files: List[os.PathLike]) -> None:\n",
    "        \"\"\"\n",
    "        Fits the vectorizer to the given text files\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text_files : List[os.PathLike]\n",
    "            List of paths to the text files to fit the vectorizer to\n",
    "        \"\"\"\n",
    "        if self.how in [\"tfidf\", \"bow\"]:\n",
    "            self.vectorizer.fit(text_files)\n",
    "            self.vectorizer.input = \"content\"\n",
    "        elif self.how in [\"bert\", \"roberta\"]:\n",
    "            pass\n",
    "        else:\n",
    "            raise NotImplementedError(\"how must be either tfidf, bow or bert\")\n",
    "\n",
    "        self.trained = True\n",
    "\n",
    "    def transform(self, text: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Transforms the given text into a vector\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text : str\n",
    "            Text to transform\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Array containing the vector\n",
    "        \"\"\"\n",
    "        if not self.trained:\n",
    "            raise ValueError(\"Vectorizer must be trained first\")\n",
    "        if self.how in [\"tfidf\", \"bow\"]:\n",
    "            vector = self.vectorizer.transform([text]).toarray()\n",
    "        elif self.how == \"bert\":\n",
    "            tokens = self.tokenizer(\n",
    "                [text], padding='max_length', max_length=1, truncation=True, return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.vectorizer(**tokens)\n",
    "                vector = outputs.last_hidden_state[0].numpy()\n",
    "        elif self.how == \"roberta\":\n",
    "            return self.vectorizer.encode([text])\n",
    "\n",
    "        return vector\n",
    "\n",
    "\n",
    "def fit_vectorizer(\n",
    "    username: str,\n",
    "    root: str = \"../data\",\n",
    "    how: Union[str, \"tfidf\", \"bow\", \"bert\"] = \"tfidf\",\n",
    "    ngram_range: Tuple[int, int] = (1, 1),\n",
    "    fit_before: Union[str, datetime] = datetime.today(),\n",
    "    fit_after: datetime = datetime(year=2000, month=1, day = 1),\n",
    "    batch_size: int = 8,\n",
    ") -> Tuple[np.ndarray, TfidfVectorizer]:\n",
    "    \"\"\"\n",
    "    Vectorizes the content of a given profile. Assumes the download\n",
    "    has already been done, and the directory is full of posts. Directory\n",
    "    is expected to contain a folder named after the profile, which contains\n",
    "    the text files and json metadata files for each post.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    username : str\n",
    "        Username of the profile to vectorize\n",
    "    root : str\n",
    "        Path to the folder where the posts are stored\n",
    "    how : str\n",
    "        How to vectorize the content. Can be either \"tfidf\", \"bow\" (bag of words), or \"bert\"\n",
    "    return_vectorizer : bool\n",
    "        Whether to return the vectorizer object or not\n",
    "    ngram_range : Tuple[int, int]\n",
    "        Range of ngrams to use for tfidf or count vectorization\n",
    "    fit_before : Union[str, datetime]\n",
    "        Date to fit the vectorizer before. Can be either a datetime object\n",
    "    batch_size : int\n",
    "        Batch size for bert vectorization\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[np.ndarray, Union[TfidfVectorizer, None]]\n",
    "        Tuple containing the vectors and the vectorizer object if\n",
    "        return_vectorizer is True, None otherwise\n",
    "    \"\"\"\n",
    "    profile_path = os.path.join(root, username)\n",
    "    text_files = [\n",
    "        os.path.join(profile_path, file)\n",
    "        for file in os.listdir(profile_path)\n",
    "        if (file.endswith(\"UCT.txt\") and\n",
    "            fit_after < \n",
    "            datetime.strptime(os.path.basename(file), \"%Y-%m-%d_%H-%M-%S_UTC.txt\")\n",
    "            <= fit_before\n",
    "           )\n",
    "    ]\n",
    "    print(fit_before)\n",
    "    print(fit_after)\n",
    "    print(text_files)\n",
    "\n",
    "    vectorizer = Vectorizer(how=how, ngram_range=ngram_range)\n",
    "    vectorizer.fit(text_files)\n",
    "\n",
    "    # Necessary to work with raw text inputs after training on documents\n",
    "    return vectorizer\n",
    "\n",
    "\n",
    "def remove_control_characters(html: str) -> str:\n",
    "    \"\"\"\n",
    "    Strip invalid XML characters that `lxml` cannot parse.\n",
    "    See: https://github.com/html5lib/html5lib-python/issues/96\n",
    "\n",
    "    The XML 1.0 spec defines the valid character range as:\n",
    "    Char ::= #x9 | #xA | #xD | [#x20-#xD7FF] | [#xE000-#xFFFD] | [#x10000-#x10FFFF]\n",
    "\n",
    "    We can instead match the invalid characters by inverting that range into:\n",
    "    InvalidChar ::= #xb | #xc | #xFFFE | #xFFFF | [#x0-#x8] | [#xe-#x1F] | [#xD800-#xDFFF]\n",
    "\n",
    "    Sources:\n",
    "    https://www.w3.org/TR/REC-xml/#charsets,\n",
    "    https://lsimons.wordpress.com/2011/03/17/stripping-illegal-characters-out-of-xml-in-python/\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    html : str\n",
    "        HTML string to clean\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Cleaned HTML string\n",
    "    \"\"\"\n",
    "\n",
    "    def strip_illegal_xml_characters(s, default, base=10):\n",
    "        # Compare the \"invalid XML character range\" numerically\n",
    "        n = int(s, base)\n",
    "        if (\n",
    "            n in (0xB, 0xC, 0xFFFE, 0xFFFF)\n",
    "            or 0x0 <= n <= 0x8\n",
    "            or 0xE <= n <= 0x1F\n",
    "            or 0xD800 <= n <= 0xDFFF\n",
    "        ):\n",
    "            return \"\"\n",
    "        return default\n",
    "\n",
    "    # We encode all non-ascii characters to XML char-refs, so for example \"💖\" becomes: \"&#x1F496;\"\n",
    "    # Otherwise we'd remove emojis by mistake on narrow-unicode builds of Python\n",
    "    html = html.encode(\"ascii\", \"xmlcharrefreplace\").decode(\"utf-8\")\n",
    "    html = re.sub(\n",
    "        r\"&#(\\d+);?\",\n",
    "        lambda c: strip_illegal_xml_characters(c.group(1), c.group(0)),\n",
    "        html,\n",
    "    )\n",
    "    html = re.sub(\n",
    "        r\"&#[xX]([0-9a-fA-F]+);?\",\n",
    "        lambda c: strip_illegal_xml_characters(c.group(1), c.group(0), base=16),\n",
    "        html,\n",
    "    )\n",
    "    # A regex matching the \"invalid XML character range\"\n",
    "    html = re.compile(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1F\\uD800-\\uDFFF\\uFFFE\\uFFFF]\").sub(\n",
    "        \"\", html\n",
    "    )\n",
    "    return html\n",
    "\n",
    "\n",
    "def get_article(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Obtains the likely text from an article based on the newspaper library\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url: str\n",
    "        URL of the article to fetch\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Most likely article text\n",
    "    \"\"\"\n",
    "\n",
    "    text = \"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4644.45 Safari/537.36\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        article = Article(url)\n",
    "        article.download(input_html=remove_control_characters(response.text))\n",
    "        article.parse()\n",
    "        text = article.text\n",
    "\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        pass\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_trends(\n",
    "    hl=\"en-US\",\n",
    "    geo=\"US\",\n",
    "    tz=360,\n",
    "    count=20,\n",
    "    date: Union[datetime, str] = datetime.today(),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch Google Trends realtime data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hl : str\n",
    "        Language\n",
    "    geo : str\n",
    "        Country\n",
    "    tz : int\n",
    "        Timezone\n",
    "    count : int\n",
    "        Number of results\n",
    "    date : Union[datetime, str]\n",
    "        Date to fetch data from. Can be either a datetime object or a string\n",
    "        in the format YYYY-MM-DD (fuzzy parsing is enabled, but not recommended)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing the results\n",
    "    \"\"\"\n",
    "    if isinstance(date, str):\n",
    "        date = dateparser.parse(date, fuzzy=True)\n",
    "\n",
    "    response = requests.get(\n",
    "        \"https://trends.google.com/trends/api/dailytrends\",\n",
    "        params={\n",
    "            \"hl\": hl,\n",
    "            \"tz\": tz,\n",
    "            \"ed\": date.strftime(\"%Y%m%d\"),\n",
    "            \"geo\": geo,\n",
    "            \"ns\": count,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    response.raise_for_status()\n",
    "    if response.status_code != 204:\n",
    "        data = response.text.split(\")]}',\\n\")[1]\n",
    "        data = json.loads(data)[\"default\"][\"trendingSearchesDays\"][0][\n",
    "            \"trendingSearches\"\n",
    "        ]\n",
    "\n",
    "    dfs = pd.concat(\n",
    "        [pd.DataFrame(trend[\"articles\"]) for trend in data], ignore_index=True\n",
    "    )\n",
    "\n",
    "    dfs[\"text\"] = dfs[\"url\"].apply(get_article)\n",
    "\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def read_metadata_json(fp: os.PathLike):\n",
    "    \"\"\"\n",
    "    Reads a json file containing the metadata of an Instagram post\n",
    "    and returns a dictionary with the relevant information. Expects\n",
    "    the file to be named as YYYY-MM-DD_HH-MM-SS_UTC.json, which is\n",
    "    the default for instaloader.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fp : os.PathLike\n",
    "        Path to the json file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary containing the relevant metadata\n",
    "    \"\"\"\n",
    "    with open(fp, \"r\") as f:\n",
    "        metadata = json.loads(f.read())[\"node\"]\n",
    "\n",
    "        dt = datetime.strptime(os.path.basename(fp), \"%Y-%m-%d_%H-%M-%S_UTC.json\")\n",
    "\n",
    "        clean_metadata = {\n",
    "            \"dt\": dt,\n",
    "            \"likes\": metadata[\"edge_media_preview_like\"][\"count\"],\n",
    "            \"comments\": metadata[\"edge_media_to_comment\"][\"count\"],\n",
    "            \"caption\": metadata[\"edge_media_to_caption\"][\"edges\"][0][\"node\"][\"text\"]\n",
    "            if metadata[\"edge_media_to_caption\"][\"edges\"]\n",
    "            else \"\",\n",
    "            \"comments_disabled\": metadata[\"comments_disabled\"],\n",
    "            \"is_video\": metadata[\"is_video\"],\n",
    "            \"tagged_users\": metadata[\"edge_media_to_tagged_user\"],\n",
    "        }\n",
    "\n",
    "        return clean_metadata\n",
    "\n",
    "\n",
    "def get_posts(\n",
    "    username: str,\n",
    "    root: os.PathLike = \"../data\",\n",
    "    get_before: Union[str, datetime] = datetime.today(),\n",
    "    get_after: datetime = datetime(year=2000, month=1, day = 1),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads all the posts of a given profile and returns a DataFrame\n",
    "    with the relevant information\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    username : str\n",
    "        Username of the profile to read\n",
    "    root : os.PathLike\n",
    "        Path to the folder where the posts are stored\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing the relevant metadata\n",
    "    \"\"\"\n",
    "    profile_path = os.path.join(root, username)\n",
    "    json_files = [\n",
    "        os.path.join(profile_path, file)\n",
    "        for file in os.listdir(profile_path)\n",
    "        if file.endswith(\"UTC.json\") and\n",
    "        get_after < \n",
    "        datetime.strptime(os.path.basename(file), \"%Y-%m-%d_%H-%M-%S_UTC.json\")\n",
    "        <= get_before       \n",
    "    ]\n",
    "    metadata = [read_metadata_json(file) for file in json_files]\n",
    "    df = pd.DataFrame(metadata)\n",
    "    df.set_index(\"dt\", inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def ema(data: pd.Series, alpha: float = 0.99) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculates the exponential moving average of a given series\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.Series\n",
    "        Series to calculate the ema for\n",
    "    alpha : float\n",
    "        Alpha parameter for the ema calculation\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        Series containing the ema values\n",
    "    \"\"\"\n",
    "    ema = []\n",
    "    ema_value = None\n",
    "\n",
    "    for value in data:\n",
    "        if ema_value is None:\n",
    "            ema_value = value\n",
    "        else:\n",
    "            ema_value = (value - ema_value) * alpha + ema_value\n",
    "        ema.append(ema_value)\n",
    "\n",
    "    return ema\n",
    "\n",
    "def calc_embedding(x: pd.Series, how: str = 'mean'):\n",
    "    if how == 'mean':\n",
    "        return np.array(x[\"embeddings\"].to_list()).mean(axis=0)\n",
    "    elif how == 'weighted':\n",
    "        weights = x[['likes', 'comments']].sum(axis=1)\n",
    "        embeddings = np.array(x[\"embeddings\"].to_list())\n",
    "        weighted_mean = np.average(embeddings, axis=0, weights=weights)\n",
    "        return weighted_mean\n",
    "\n",
    "\n",
    "def jaccard_similarity(str1, str2):\n",
    "    set1 = set(nltk.word_tokenize(str1.lower()))\n",
    "    set2 = set(nltk.word_tokenize(str2.lower()))\n",
    "    \n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    jaccard_similarity = intersection / union if union != 0 else 0.0\n",
    "    \n",
    "    return jaccard_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76040e0f-63b3-4be3-b48c-95e63b87586c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_nov = pd.read_csv(\"../Historical_Articles.csv\")\n",
    "df_pre_nov['Date'] = pd.to_datetime(df_pre_nov['Date'], format='%Y/%m/%d')\n",
    "df_pre_nov = df_pre_nov[['Date', 'article_content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f9c037-67fc-4829-a08f-337046c89c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nov = pd.read_csv(\"../entire_nov.csv\")\n",
    "df_nov['publishedAt'] = pd.to_datetime(df_nov['publishedAt'])\n",
    "df_nov = df_nov[['publishedAt', 'article_content']]\n",
    "df_nov.columns = ['Date', 'article_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b22e5c-0a84-4746-8c64-ae1f0b6198e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_trends = pd.concat([df_pre_nov, df_nov])\n",
    "google_trends['Date'] = pd.to_datetime(google_trends['Date'], utc=True) \n",
    "google_trends['Date'] = google_trends['Date'].dt.tz_localize(None)\n",
    "\n",
    "google_trends = google_trends.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ee2a7d-4798-4bfc-8e2d-d370d71d538f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "ROOT = \"../data\"\n",
    "PERIODS = [\n",
    "    # Weeks of Sep\n",
    "    ('09/01/2023', '09/08/2023'),\n",
    "    ('09/09/2023', '09/16/2023'),\n",
    "    ('09/17/2023', '09/24/2023'),\n",
    "    ('09/25/2023', '09/30/2023'),\n",
    "\n",
    "    # Weeks of Oct\n",
    "    ('10/01/2023', '10/08/2023'),\n",
    "    ('10/09/2023', '10/16/2023'),\n",
    "    ('10/17/2023', '10/24/2023'),\n",
    "    ('10/25/2023', '10/31/2023'),\n",
    "\n",
    "    # Weeks of Nov\n",
    "    ('11/01/2023', '11/08/2023'),\n",
    "    ('11/09/2023', '11/16/2023'),\n",
    "    ('11/17/2023', '11/24/2023'),\n",
    "    ('11/25/2023', '11/30/2023'),\n",
    "\n",
    "    # Bi-weeks of Sep\n",
    "    ('09/01/2023', '09/15/2023'),\n",
    "    ('09/16/2023', '09/30/2023'),\n",
    "\n",
    "    # Bi-weeks of Oct\n",
    "    ('10/01/2023', '10/15/2023'),\n",
    "    ('10/16/2023', '10/31/2023'),\n",
    "\n",
    "    # Bi-weeks of Nov\n",
    "    ('11/01/2023', '11/15/2023'),\n",
    "    ('11/16/2023', '11/30/2023'),\n",
    "]\n",
    "SIMILARITIES = {\n",
    "    'cosine': cosine_similarity,\n",
    "    'manhattan':manhattan_distances, \n",
    "    'euclidean': euclidean_distances\n",
    "}\n",
    "\n",
    "for METHOD in ['roberta', 'tfidf', 'bow']:\n",
    "    for USERNAME in ['espn', 'pubitysport', 'enews', 'forbes', 'bloomberg']:\n",
    "        for SIMILARITY in SIMILARITIES.keys():\n",
    "            for N_PER_DATE in [1,5]:\n",
    "                for EMBEDDING_METRIC in ['weighted', 'mean']:\n",
    "                    for START, END in PERIODS:\n",
    "                        print(USERNAME, METHOD, SIMILARITY, EMBEDDING_METRIC, N_PER_DATE, START, END)\n",
    "                        try:\n",
    "                            START, END = datetime.strptime(START, '%m/%d/%Y'), datetime.strptime(END, '%m/%d/%Y')\n",
    "                            profile_path = os.path.join(ROOT, USERNAME)\n",
    "                            text_files = [\n",
    "                                os.path.join(profile_path, file)\n",
    "                                for file in os.listdir(profile_path)\n",
    "                                if file.endswith(\"UTC.txt\")\n",
    "                                and datetime(year=2000, month=1, day=1) < \n",
    "                                datetime.strptime(os.path.basename(file), \"%Y-%m-%d_%H-%M-%S_UTC.txt\")\n",
    "                                < START\n",
    "                            ]\n",
    "                            \n",
    "                            # Calculate time to vectorize, and time to embed\n",
    "                            # Also calculate number of posts in that timeframe\n",
    "                            # Use Bloomberg as an example week \n",
    "                            vectorizer = Vectorizer(how=METHOD)\n",
    "                            start = time.time()\n",
    "                            vectorizer.fit(text_files)\n",
    "                            end = time.time()\n",
    "                            vectorization_time = end - start \n",
    "                            \n",
    "                            prev_posts = get_posts(USERNAME, ROOT, get_before=START)\n",
    "                            posts = get_posts(USERNAME, ROOT, get_before=END, get_after=START)\n",
    "                            articles = google_trends[(google_trends['Date'] >= START) & (google_trends['Date'] <= END)]\n",
    "                            \n",
    "                            start = time.time()\n",
    "                            tqdm.pandas(desc=\"Embed previous captions\")\n",
    "                            prev_posts[\"embeddings\"] = prev_posts[\"caption\"].progress_apply(vectorizer.transform)\n",
    "                            embedding = calc_embedding(prev_posts, how=EMBEDDING_METRIC)\n",
    "                            end = time.time()\n",
    "                            embed_captions_time = end - start \n",
    "                            \n",
    "                            start = time.time()\n",
    "                            tqdm.pandas(desc=\"Embed articles\")\n",
    "                            articles[\"embeddings\"] = articles[\"article_content\"].progress_apply(vectorizer.transform)\n",
    "                            end = time.time()\n",
    "                            embed_articles_time = end - start \n",
    "                            \n",
    "                            articles[\"similarity\"] = articles[\"embeddings\"].apply(\n",
    "                                lambda x: SIMILARITIES[SIMILARITY](embedding, x)[0][0]\n",
    "                            )\n",
    "                            \n",
    "                            # Get top posts per day and calculate jaccard \n",
    "                            top_k = articles.loc[articles.groupby(articles[\"Date\"])[\"similarity\"].nlargest(1).index.levels[1]]\n",
    "                            \n",
    "                            article_text = top_k['article_content'].str.cat(sep=' ')\n",
    "                            posts_text = posts['caption'].str.cat(sep=' ')\n",
    "                            jaccard_score = jaccard_similarity(article_text, posts_text)\n",
    "                            \n",
    "                            results.append({\n",
    "                                \"username\": USERNAME,\n",
    "                                \"method\": METHOD,\n",
    "                                \"n_per_date\": N_PER_DATE,\n",
    "                                \"similarity\": SIMILARITY,\n",
    "                                \"embedding_metric\": EMBEDDING_METRIC,\n",
    "                                \"num_previous_posts\": len(prev_posts),\n",
    "                                \"num_timeframe_posts\": len(posts),\n",
    "                                \"num_articles\": len(articles),\n",
    "                                \"start\": START,\n",
    "                                \"end\": END,\n",
    "                                \"vectorization_time\": vectorization_time,\n",
    "                                \"embed_articles_time\": embed_articles_time,\n",
    "                                \"embed_captions_time\": embed_captions_time,\n",
    "                                \"jaccard\": jaccard_score\n",
    "                            })\n",
    "                            pd.DataFrame(results).to_csv(\"FINAL_RESULTS.csv\")\n",
    "                            print(results[-1])\n",
    "                        except Exception as E:\n",
    "                            print(f\"Failed to calculate Jaccard: {E}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
