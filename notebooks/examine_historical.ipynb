{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "407cf7e5-190d-47f8-ae73-ef39c6a85219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "import instaloader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "from dateutil import parser as dateparser\n",
    "from newspaper import Article\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import nltk\n",
    "import time\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity, manhattan_distances, euclidean_distances\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "def download_profile(\n",
    "    usernames: Union[str, List[str]], root: os.PathLike = \"../data\", **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Downloads all posts of a given publicly-accessible profile.\n",
    "    Does not download images or videos, only metadata\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    usernames : str, List[str]\n",
    "        Username(s) of the profile to download\n",
    "    root : str\n",
    "        Path to the folder where the posts will be stored\n",
    "    \"\"\"\n",
    "\n",
    "    loader = instaloader.Instaloader(\n",
    "        dirname_pattern=os.path.join(root, \"{profile}\"),\n",
    "        download_pictures=False,\n",
    "        download_videos=False,\n",
    "        download_video_thumbnails=False,\n",
    "        download_geotags=False,\n",
    "        download_comments=False,\n",
    "        save_metadata=True,\n",
    "        compress_json=False,\n",
    "    )\n",
    "\n",
    "    if isinstance(usernames, str):\n",
    "        usernames = [usernames]\n",
    "\n",
    "    profiles = [\n",
    "        instaloader.Profile.from_username(loader.context, username)\n",
    "        for username in usernames\n",
    "    ]\n",
    "\n",
    "    latest_stamps = instaloader.LatestStamps(os.path.join(root, \"latest_timestamp.txt\"))\n",
    "\n",
    "    loader.download_profiles(\n",
    "        profiles,\n",
    "        fast_update=True,\n",
    "        profile_pic=False,\n",
    "        igtv=False,\n",
    "        latest_stamps=latest_stamps,\n",
    "        stories=False,\n",
    "        highlights=False,\n",
    "        tagged=False,\n",
    "    )\n",
    "\n",
    "\n",
    "class Vectorizer:\n",
    "    \"\"\"\n",
    "    A class for vectorizing text inputs\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, how: str = \"tfidf\", ngram_range: Tuple[int, int] = (1, 1)):\n",
    "        \"\"\"\n",
    "        Initializes the vectorizer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        how : str\n",
    "            How to vectorize the content. Can be either \"tfidf\", \"bow\" (bag of words), or \"bert\"\n",
    "        ngram_range : Tuple[int, int]\n",
    "            Range of ngrams to use for tfidf or count vectorization\n",
    "        \"\"\"\n",
    "        self.how = how\n",
    "        self.ngram_range = ngram_range\n",
    "        if self.how in [\"tfidf\", \"bow\"]:\n",
    "            self.vectorizer = (\n",
    "                TfidfVectorizer if self.how == \"tfidf\" else CountVectorizer\n",
    "            )(\n",
    "                input=\"filename\",\n",
    "                strip_accents=\"unicode\",\n",
    "                ngram_range=self.ngram_range,\n",
    "            )\n",
    "        elif self.how == \"bert\":\n",
    "            self.vectorizer = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                \"vinai/bertweet-base\", use_fast=False\n",
    "            )\n",
    "        elif self.how == \"roberta\":\n",
    "            self.vectorizer = SentenceTransformer(\"all-distilroberta-v1\")\n",
    "        else:\n",
    "            raise NotImplementedError(\"how must be either tfidf, bow or bert\")\n",
    "        self.trained = False\n",
    "\n",
    "    def fit_transform(\n",
    "        self, text_files: List[os.PathLike], batch_size: int = 8\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Fits the vectorizer to the given text files, and returns the vectors\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text_files : List[os.PathLike]\n",
    "            List of paths to the text files to fit the vectorizer to\n",
    "        batch_size : int\n",
    "            Batch size for bert vectorization\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Array containing the vectors\n",
    "        \"\"\"\n",
    "        if self.how in [\"tfidf\", \"bow\"]:\n",
    "            vectors = self.vectorizer.fit_transform(text_files).toarray()\n",
    "            self.vectorizer.input = \"content\"\n",
    "        elif self.how == \"bert\":\n",
    "            all_embeddings = []\n",
    "\n",
    "            for i in tqdm(range(0, len(text_files), 8), desc=\"Bert vectorization\"):\n",
    "                batch_contents = [\n",
    "                    open(file_path, \"r\", encoding=\"utf-8\").read()\n",
    "                    for file_path in text_files[i : i + 8]\n",
    "                    if os.path.exists(file_path)\n",
    "                ]\n",
    "\n",
    "                tokens = self.tokenizer(\n",
    "                    batch_contents, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "                )\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.vectorizer(**tokens)\n",
    "                    embeddings = [\n",
    "                        o.numpy() for o in outputs.last_hidden_state\n",
    "                    ]  # This contains the embeddings for each token in the input\n",
    "                    all_embeddings.extend(embeddings)\n",
    "\n",
    "            vectors = np.array(all_embeddings)\n",
    "        else:\n",
    "            raise NotImplementedError(\"how must be either tfidf, bow or bert\")\n",
    "\n",
    "        self.trained = True\n",
    "        return vectors\n",
    "\n",
    "    def fit(self, text_files: List[os.PathLike]) -> None:\n",
    "        \"\"\"\n",
    "        Fits the vectorizer to the given text files\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text_files : List[os.PathLike]\n",
    "            List of paths to the text files to fit the vectorizer to\n",
    "        \"\"\"\n",
    "        if self.how in [\"tfidf\", \"bow\"]:\n",
    "            self.vectorizer.fit(text_files)\n",
    "            self.vectorizer.input = \"content\"\n",
    "        elif self.how in [\"bert\", \"roberta\"]:\n",
    "            pass\n",
    "        else:\n",
    "            raise NotImplementedError(\"how must be either tfidf, bow or bert\")\n",
    "\n",
    "        self.trained = True\n",
    "\n",
    "    def transform(self, text: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Transforms the given text into a vector\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text : str\n",
    "            Text to transform\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Array containing the vector\n",
    "        \"\"\"\n",
    "        if not self.trained:\n",
    "            raise ValueError(\"Vectorizer must be trained first\")\n",
    "        if self.how in [\"tfidf\", \"bow\"]:\n",
    "            vector = self.vectorizer.transform([text]).toarray()\n",
    "        elif self.how == \"bert\":\n",
    "            tokens = self.tokenizer(\n",
    "                [text], padding='max_length', max_length=1, truncation=True, return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.vectorizer(**tokens)\n",
    "                vector = outputs.last_hidden_state[0].numpy()\n",
    "        elif self.how == \"roberta\":\n",
    "            return self.vectorizer.encode([text])\n",
    "\n",
    "        return vector\n",
    "\n",
    "\n",
    "def fit_vectorizer(\n",
    "    username: str,\n",
    "    root: str = \"../data\",\n",
    "    how: Union[str, \"tfidf\", \"bow\", \"bert\"] = \"tfidf\",\n",
    "    ngram_range: Tuple[int, int] = (1, 1),\n",
    "    fit_before: Union[str, datetime] = datetime.today(),\n",
    "    fit_after: datetime = datetime(year=2000, month=1, day = 1),\n",
    "    batch_size: int = 8,\n",
    ") -> Tuple[np.ndarray, TfidfVectorizer]:\n",
    "    \"\"\"\n",
    "    Vectorizes the content of a given profile. Assumes the download\n",
    "    has already been done, and the directory is full of posts. Directory\n",
    "    is expected to contain a folder named after the profile, which contains\n",
    "    the text files and json metadata files for each post.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    username : str\n",
    "        Username of the profile to vectorize\n",
    "    root : str\n",
    "        Path to the folder where the posts are stored\n",
    "    how : str\n",
    "        How to vectorize the content. Can be either \"tfidf\", \"bow\" (bag of words), or \"bert\"\n",
    "    return_vectorizer : bool\n",
    "        Whether to return the vectorizer object or not\n",
    "    ngram_range : Tuple[int, int]\n",
    "        Range of ngrams to use for tfidf or count vectorization\n",
    "    fit_before : Union[str, datetime]\n",
    "        Date to fit the vectorizer before. Can be either a datetime object\n",
    "    batch_size : int\n",
    "        Batch size for bert vectorization\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[np.ndarray, Union[TfidfVectorizer, None]]\n",
    "        Tuple containing the vectors and the vectorizer object if\n",
    "        return_vectorizer is True, None otherwise\n",
    "    \"\"\"\n",
    "    profile_path = os.path.join(root, username)\n",
    "    text_files = [\n",
    "        os.path.join(profile_path, file)\n",
    "        for file in os.listdir(profile_path)\n",
    "        if (file.endswith(\"UCT.txt\") and\n",
    "            fit_after < \n",
    "            datetime.strptime(os.path.basename(file), \"%Y-%m-%d_%H-%M-%S_UTC.txt\")\n",
    "            <= fit_before\n",
    "           )\n",
    "    ]\n",
    "    print(fit_before)\n",
    "    print(fit_after)\n",
    "    print(text_files)\n",
    "\n",
    "    vectorizer = Vectorizer(how=how, ngram_range=ngram_range)\n",
    "    vectorizer.fit(text_files)\n",
    "\n",
    "    # Necessary to work with raw text inputs after training on documents\n",
    "    return vectorizer\n",
    "\n",
    "\n",
    "def remove_control_characters(html: str) -> str:\n",
    "    \"\"\"\n",
    "    Strip invalid XML characters that `lxml` cannot parse.\n",
    "    See: https://github.com/html5lib/html5lib-python/issues/96\n",
    "\n",
    "    The XML 1.0 spec defines the valid character range as:\n",
    "    Char ::= #x9 | #xA | #xD | [#x20-#xD7FF] | [#xE000-#xFFFD] | [#x10000-#x10FFFF]\n",
    "\n",
    "    We can instead match the invalid characters by inverting that range into:\n",
    "    InvalidChar ::= #xb | #xc | #xFFFE | #xFFFF | [#x0-#x8] | [#xe-#x1F] | [#xD800-#xDFFF]\n",
    "\n",
    "    Sources:\n",
    "    https://www.w3.org/TR/REC-xml/#charsets,\n",
    "    https://lsimons.wordpress.com/2011/03/17/stripping-illegal-characters-out-of-xml-in-python/\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    html : str\n",
    "        HTML string to clean\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Cleaned HTML string\n",
    "    \"\"\"\n",
    "\n",
    "    def strip_illegal_xml_characters(s, default, base=10):\n",
    "        # Compare the \"invalid XML character range\" numerically\n",
    "        n = int(s, base)\n",
    "        if (\n",
    "            n in (0xB, 0xC, 0xFFFE, 0xFFFF)\n",
    "            or 0x0 <= n <= 0x8\n",
    "            or 0xE <= n <= 0x1F\n",
    "            or 0xD800 <= n <= 0xDFFF\n",
    "        ):\n",
    "            return \"\"\n",
    "        return default\n",
    "\n",
    "    # We encode all non-ascii characters to XML char-refs, so for example \"💖\" becomes: \"&#x1F496;\"\n",
    "    # Otherwise we'd remove emojis by mistake on narrow-unicode builds of Python\n",
    "    html = html.encode(\"ascii\", \"xmlcharrefreplace\").decode(\"utf-8\")\n",
    "    html = re.sub(\n",
    "        r\"&#(\\d+);?\",\n",
    "        lambda c: strip_illegal_xml_characters(c.group(1), c.group(0)),\n",
    "        html,\n",
    "    )\n",
    "    html = re.sub(\n",
    "        r\"&#[xX]([0-9a-fA-F]+);?\",\n",
    "        lambda c: strip_illegal_xml_characters(c.group(1), c.group(0), base=16),\n",
    "        html,\n",
    "    )\n",
    "    # A regex matching the \"invalid XML character range\"\n",
    "    html = re.compile(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1F\\uD800-\\uDFFF\\uFFFE\\uFFFF]\").sub(\n",
    "        \"\", html\n",
    "    )\n",
    "    return html\n",
    "\n",
    "\n",
    "def get_article(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Obtains the likely text from an article based on the newspaper library\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url: str\n",
    "        URL of the article to fetch\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Most likely article text\n",
    "    \"\"\"\n",
    "\n",
    "    text = \"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4644.45 Safari/537.36\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        article = Article(url)\n",
    "        article.download(input_html=remove_control_characters(response.text))\n",
    "        article.parse()\n",
    "        text = article.text\n",
    "\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        pass\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_trends(\n",
    "    hl=\"en-US\",\n",
    "    geo=\"US\",\n",
    "    tz=360,\n",
    "    count=20,\n",
    "    date: Union[datetime, str] = datetime.today(),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch Google Trends realtime data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hl : str\n",
    "        Language\n",
    "    geo : str\n",
    "        Country\n",
    "    tz : int\n",
    "        Timezone\n",
    "    count : int\n",
    "        Number of results\n",
    "    date : Union[datetime, str]\n",
    "        Date to fetch data from. Can be either a datetime object or a string\n",
    "        in the format YYYY-MM-DD (fuzzy parsing is enabled, but not recommended)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing the results\n",
    "    \"\"\"\n",
    "    if isinstance(date, str):\n",
    "        date = dateparser.parse(date, fuzzy=True)\n",
    "\n",
    "    response = requests.get(\n",
    "        \"https://trends.google.com/trends/api/dailytrends\",\n",
    "        params={\n",
    "            \"hl\": hl,\n",
    "            \"tz\": tz,\n",
    "            \"ed\": date.strftime(\"%Y%m%d\"),\n",
    "            \"geo\": geo,\n",
    "            \"ns\": count,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    response.raise_for_status()\n",
    "    if response.status_code != 204:\n",
    "        data = response.text.split(\")]}',\\n\")[1]\n",
    "        data = json.loads(data)[\"default\"][\"trendingSearchesDays\"][0][\n",
    "            \"trendingSearches\"\n",
    "        ]\n",
    "\n",
    "    dfs = pd.concat(\n",
    "        [pd.DataFrame(trend[\"articles\"]) for trend in data], ignore_index=True\n",
    "    )\n",
    "\n",
    "    dfs[\"text\"] = dfs[\"url\"].apply(get_article)\n",
    "\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def read_metadata_json(fp: os.PathLike):\n",
    "    \"\"\"\n",
    "    Reads a json file containing the metadata of an Instagram post\n",
    "    and returns a dictionary with the relevant information. Expects\n",
    "    the file to be named as YYYY-MM-DD_HH-MM-SS_UTC.json, which is\n",
    "    the default for instaloader.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fp : os.PathLike\n",
    "        Path to the json file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary containing the relevant metadata\n",
    "    \"\"\"\n",
    "    with open(fp, \"r\") as f:\n",
    "        metadata = json.loads(f.read())[\"node\"]\n",
    "\n",
    "        dt = datetime.strptime(os.path.basename(fp), \"%Y-%m-%d_%H-%M-%S_UTC.json\")\n",
    "\n",
    "        clean_metadata = {\n",
    "            \"dt\": dt,\n",
    "            \"likes\": metadata[\"edge_media_preview_like\"][\"count\"],\n",
    "            \"comments\": metadata[\"edge_media_to_comment\"][\"count\"],\n",
    "            \"caption\": metadata[\"edge_media_to_caption\"][\"edges\"][0][\"node\"][\"text\"]\n",
    "            if metadata[\"edge_media_to_caption\"][\"edges\"]\n",
    "            else \"\",\n",
    "            \"comments_disabled\": metadata[\"comments_disabled\"],\n",
    "            \"is_video\": metadata[\"is_video\"],\n",
    "            \"tagged_users\": metadata[\"edge_media_to_tagged_user\"],\n",
    "        }\n",
    "\n",
    "        return clean_metadata\n",
    "\n",
    "\n",
    "def get_posts(\n",
    "    username: str,\n",
    "    root: os.PathLike = \"../data\",\n",
    "    get_before: Union[str, datetime] = datetime.today(),\n",
    "    get_after: datetime = datetime(year=2000, month=1, day = 1),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads all the posts of a given profile and returns a DataFrame\n",
    "    with the relevant information\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    username : str\n",
    "        Username of the profile to read\n",
    "    root : os.PathLike\n",
    "        Path to the folder where the posts are stored\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing the relevant metadata\n",
    "    \"\"\"\n",
    "    profile_path = os.path.join(root, username)\n",
    "    json_files = [\n",
    "        os.path.join(profile_path, file)\n",
    "        for file in os.listdir(profile_path)\n",
    "        if file.endswith(\"UTC.json\") and\n",
    "        get_after < \n",
    "        datetime.strptime(os.path.basename(file), \"%Y-%m-%d_%H-%M-%S_UTC.json\")\n",
    "        <= get_before       \n",
    "    ]\n",
    "    metadata = [read_metadata_json(file) for file in json_files]\n",
    "    df = pd.DataFrame(metadata)\n",
    "    df.set_index(\"dt\", inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def ema(data: pd.Series, alpha: float = 0.99) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculates the exponential moving average of a given series\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.Series\n",
    "        Series to calculate the ema for\n",
    "    alpha : float\n",
    "        Alpha parameter for the ema calculation\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        Series containing the ema values\n",
    "    \"\"\"\n",
    "    ema = []\n",
    "    ema_value = None\n",
    "\n",
    "    for value in data:\n",
    "        if ema_value is None:\n",
    "            ema_value = value\n",
    "        else:\n",
    "            ema_value = (value - ema_value) * alpha + ema_value\n",
    "        ema.append(ema_value)\n",
    "\n",
    "    return ema\n",
    "\n",
    "def calc_embedding(x: pd.Series, how: str = 'mean'):\n",
    "    if how == 'mean':\n",
    "        return np.array(x.to_list()).mean(axis=0)\n",
    "\n",
    "\n",
    "def jaccard_similarity(str1, str2):\n",
    "    set1 = set(nltk.word_tokenize(str1.lower()))\n",
    "    set2 = set(nltk.word_tokenize(str2.lower()))\n",
    "    \n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    jaccard_similarity = intersection / union if union != 0 else 0.0\n",
    "    \n",
    "    return jaccard_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76040e0f-63b3-4be3-b48c-95e63b87586c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_nov = pd.read_csv(\"../Historical_Articles.csv\")\n",
    "df_pre_nov['Date'] = pd.to_datetime(df_pre_nov['Date'], format='%Y/%m/%d')\n",
    "df_pre_nov = df_pre_nov[['Date', 'article_content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4f9c037-67fc-4829-a08f-337046c89c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nov = pd.read_csv(\"../entire_nov.csv\")\n",
    "df_nov['publishedAt'] = pd.to_datetime(df_nov['publishedAt'])\n",
    "df_nov = df_nov[['publishedAt', 'article_content']]\n",
    "df_nov.columns = ['Date', 'article_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23b22e5c-0a84-4746-8c64-ae1f0b6198e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_trends = pd.concat([df_pre_nov, df_nov])\n",
    "google_trends['Date'] = pd.to_datetime(google_trends['Date'], utc=True) \n",
    "google_trends['Date'] = google_trends['Date'].dt.tz_localize(None)\n",
    "\n",
    "google_trends = google_trends.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4ee2a7d-4798-4bfc-8e2d-d370d71d538f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "espn roberta cosine 1 09/01/2023 09/08/2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embed previous captions: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28688/28688 [08:30<00:00, 56.19it/s]\n",
      "Embed articles: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 52/52 [00:04<00:00, 10.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'username': 'espn', 'method': 'roberta', 'n_per_date': 1, 'similarity': 'cosine', 'start': datetime.datetime(2023, 9, 1, 0, 0), 'end': datetime.datetime(2023, 9, 8, 0, 0), 'vectorization_time': 1.1920928955078125e-06, 'embed_articles_time': 4.967262268066406, 'embed_captions_time': 510.66267824172974, 'jaccard': 0.1800441826215022}\n",
      "espn roberta cosine 1 09/09/2023 09/16/2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embed previous captions: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28909/28909 [09:07<00:00, 52.77it/s]\n",
      "Embed articles: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 41/41 [00:03<00:00, 10.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'username': 'espn', 'method': 'roberta', 'n_per_date': 1, 'similarity': 'cosine', 'start': datetime.datetime(2023, 9, 9, 0, 0), 'end': datetime.datetime(2023, 9, 16, 0, 0), 'vectorization_time': 5.9604644775390625e-06, 'embed_articles_time': 3.838365077972412, 'embed_captions_time': 547.8796610832214, 'jaccard': 0.1724137931034483}\n",
      "espn roberta cosine 1 09/17/2023 09/24/2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embed previous captions: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29128/29128 [09:09<00:00, 53.02it/s]\n",
      "Embed articles: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:04<00:00, 10.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'username': 'espn', 'method': 'roberta', 'n_per_date': 1, 'similarity': 'cosine', 'start': datetime.datetime(2023, 9, 17, 0, 0), 'end': datetime.datetime(2023, 9, 24, 0, 0), 'vectorization_time': 3.0994415283203125e-06, 'embed_articles_time': 4.872943162918091, 'embed_captions_time': 549.4259278774261, 'jaccard': 0.15737179487179487}\n",
      "espn roberta cosine 1 09/25/2023 09/30/2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embed previous captions: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29377/29377 [09:18<00:00, 52.59it/s]\n",
      "Embed articles: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:02<00:00, 10.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'username': 'espn', 'method': 'roberta', 'n_per_date': 1, 'similarity': 'cosine', 'start': datetime.datetime(2023, 9, 25, 0, 0), 'end': datetime.datetime(2023, 9, 30, 0, 0), 'vectorization_time': 3.0994415283203125e-06, 'embed_articles_time': 2.8172030448913574, 'embed_captions_time': 558.639811038971, 'jaccard': 0.14666136724960255}\n",
      "espn roberta cosine 1 10/01/2023 10/08/2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embed previous captions: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29535/29535 [09:24<00:00, 52.32it/s]\n",
      "Embed articles: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 37/37 [00:03<00:00, 11.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'username': 'espn', 'method': 'roberta', 'n_per_date': 1, 'similarity': 'cosine', 'start': datetime.datetime(2023, 10, 1, 0, 0), 'end': datetime.datetime(2023, 10, 8, 0, 0), 'vectorization_time': 8.106231689453125e-06, 'embed_articles_time': 3.092581033706665, 'embed_captions_time': 564.5642099380493, 'jaccard': 0.16027088036117382}\n",
      "espn roberta cosine 1 10/09/2023 10/16/2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embed previous captions: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29808/29808 [09:03<00:00, 54.82it/s]\n",
      "Embed articles: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:04<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'username': 'espn', 'method': 'roberta', 'n_per_date': 1, 'similarity': 'cosine', 'start': datetime.datetime(2023, 10, 9, 0, 0), 'end': datetime.datetime(2023, 10, 16, 0, 0), 'vectorization_time': 2.86102294921875e-06, 'embed_articles_time': 4.862525939941406, 'embed_captions_time': 543.836088180542, 'jaccard': 0.18377742946708464}\n",
      "espn roberta cosine 1 10/17/2023 10/24/2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embed previous captions: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30065/30065 [09:22<00:00, 53.43it/s]\n",
      "Embed articles: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [00:02<00:00, 10.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'username': 'espn', 'method': 'roberta', 'n_per_date': 1, 'similarity': 'cosine', 'start': datetime.datetime(2023, 10, 17, 0, 0), 'end': datetime.datetime(2023, 10, 24, 0, 0), 'vectorization_time': 2.86102294921875e-06, 'embed_articles_time': 2.1686930656433105, 'embed_captions_time': 562.7301259040833, 'jaccard': 0.14285714285714285}\n",
      "espn roberta cosine 1 10/25/2023 10/31/2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embed previous captions: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30310/30310 [09:39<00:00, 52.29it/s]\n",
      "Embed articles: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 10.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'username': 'espn', 'method': 'roberta', 'n_per_date': 1, 'similarity': 'cosine', 'start': datetime.datetime(2023, 10, 25, 0, 0), 'end': datetime.datetime(2023, 10, 31, 0, 0), 'vectorization_time': 3.0994415283203125e-06, 'embed_articles_time': 0.8390169143676758, 'embed_captions_time': 579.7291860580444, 'jaccard': 0.1056315622017181}\n",
      "espn roberta cosine 1 11/01/2023 11/08/2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embed previous captions: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30570/30570 [09:55<00:00, 51.33it/s]\n",
      "Embed articles:   6%|████████████▎                                                                                                                                                                                                    | 23/389 [00:01<00:31, 11.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to calculate Jaccard: 'float' object is not subscriptable\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 112\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m E:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to calculate Jaccard: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m(E)\n",
      "Cell \u001b[0;32mIn[6], line 81\u001b[0m\n\u001b[1;32m     79\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     80\u001b[0m tqdm\u001b[38;5;241m.\u001b[39mpandas(desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbed articles\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 81\u001b[0m articles[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43marticles\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marticle_content\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogress_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     83\u001b[0m embed_articles_time \u001b[38;5;241m=\u001b[39m end \u001b[38;5;241m-\u001b[39m start \n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_project/lib/python3.11/site-packages/tqdm/std.py:920\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[0;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;66;03m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;66;03m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 920\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_function\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    922\u001b[0m     t\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_project/lib/python3.11/site-packages/pandas/core/series.py:4760\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4626\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4627\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4632\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4633\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4634\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4635\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4636\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4751\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4752\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4753\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4754\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4755\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4756\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4757\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4758\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4760\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_project/lib/python3.11/site-packages/pandas/core/apply.py:1207\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_project/lib/python3.11/site-packages/pandas/core/apply.py:1287\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1287\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_project/lib/python3.11/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_project/lib/python3.11/site-packages/pandas/core/algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1818\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2920\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_project/lib/python3.11/site-packages/tqdm/std.py:915\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;66;03m# update tbar correctly\u001b[39;00m\n\u001b[1;32m    911\u001b[0m     \u001b[38;5;66;03m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[1;32m    912\u001b[0m     \u001b[38;5;66;03m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;66;03m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[1;32m    914\u001b[0m     t\u001b[38;5;241m.\u001b[39mupdate(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m<\u001b[39m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 204\u001b[0m, in \u001b[0;36mVectorizer.transform\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    202\u001b[0m         vector \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhow \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroberta\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vector\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_project/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:161\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start_index \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(sentences), batch_size, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatches\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[1;32m    160\u001b[0m     sentences_batch \u001b[38;5;241m=\u001b[39m sentences_sorted[start_index:start_index\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[0;32m--> 161\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     features \u001b[38;5;241m=\u001b[39m batch_to_device(features, device)\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_project/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:319\u001b[0m, in \u001b[0;36mSentenceTransformer.tokenize\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: Union[List[\u001b[38;5;28mstr\u001b[39m], List[Dict], List[Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]]):\n\u001b[1;32m    316\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;124;03m    Tokenizes the texts\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 319\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_first_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_project/lib/python3.11/site-packages/sentence_transformers/models/Transformer.py:102\u001b[0m, in \u001b[0;36mTransformer.tokenize\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    100\u001b[0m batch1, batch2 \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text_tuple \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[0;32m--> 102\u001b[0m     batch1\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtext_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    103\u001b[0m     batch2\u001b[38;5;241m.\u001b[39mappend(text_tuple[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    104\u001b[0m to_tokenize \u001b[38;5;241m=\u001b[39m [batch1, batch2]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "ROOT = \"../data\"\n",
    "PERIODS = [\n",
    "    # Weeks of Sep\n",
    "    ('09/01/2023', '09/08/2023'),\n",
    "    ('09/09/2023', '09/16/2023'),\n",
    "    ('09/17/2023', '09/24/2023'),\n",
    "    ('09/25/2023', '09/30/2023'),\n",
    "\n",
    "    # Weeks of Oct\n",
    "    ('10/01/2023', '10/08/2023'),\n",
    "    ('10/09/2023', '10/16/2023'),\n",
    "    ('10/17/2023', '10/24/2023'),\n",
    "    ('10/25/2023', '10/31/2023'),\n",
    "\n",
    "    # Weeks of Nov\n",
    "    ('11/01/2023', '11/08/2023'),\n",
    "    ('11/09/2023', '11/16/2023'),\n",
    "    ('11/17/2023', '11/24/2023'),\n",
    "    ('11/25/2023', '11/30/2023'),\n",
    "\n",
    "    # Bi-weeks of Sep\n",
    "    ('09/01/2023', '09/15/2023'),\n",
    "    ('09/16/2023', '09/30/2023'),\n",
    "\n",
    "    # Bi-weeks of Oct\n",
    "    ('10/01/2023', '10/15/2023'),\n",
    "    ('10/16/2023', '10/31/2023'),\n",
    "\n",
    "    # Bi-weeks of Nov\n",
    "    ('11/01/2023', '11/15/2023'),\n",
    "    ('11/16/2023', '11/30/2023'),\n",
    "]\n",
    "SIMILARITIES = {\n",
    "    'cosine': cosine_similarity,\n",
    "    'manhattan':manhattan_distances, \n",
    "    'euclidean': euclidean_distances\n",
    "}\n",
    "\n",
    "for METHOD in ['roberta', 'tfidf', 'bow']:\n",
    "    for USERNAME in ['espn', 'pubitysport', 'enews', 'forbes', 'bloomberg']:\n",
    "        for SIMILARITY in SIMILARITIES.keys():\n",
    "            for N_PER_DATE in [1,5]:\n",
    "                for START, END in PERIODS:\n",
    "                    print(USERNAME, METHOD, SIMILARITY, N_PER_DATE, START, END)\n",
    "                    try:\n",
    "                        START, END = datetime.strptime(START, '%m/%d/%Y'), datetime.strptime(END, '%m/%d/%Y')\n",
    "                        profile_path = os.path.join(ROOT, USERNAME)\n",
    "                        text_files = [\n",
    "                            os.path.join(profile_path, file)\n",
    "                            for file in os.listdir(profile_path)\n",
    "                            if file.endswith(\"UTC.txt\")\n",
    "                            and datetime(year=2000, month=1, day=1) < \n",
    "                            datetime.strptime(os.path.basename(file), \"%Y-%m-%d_%H-%M-%S_UTC.txt\")\n",
    "                            < START\n",
    "                        ]\n",
    "                        \n",
    "                        # Calculate time to vectorize, and time to embed\n",
    "                        # Also calculate number of posts in that timeframe\n",
    "                        # Use Bloomberg as an example week \n",
    "                        vectorizer = Vectorizer(how=METHOD)\n",
    "                        start = time.time()\n",
    "                        vectorizer.fit(text_files)\n",
    "                        end = time.time()\n",
    "                        vectorization_time = end - start \n",
    "                        \n",
    "                        prev_posts = get_posts(USERNAME, ROOT, get_before=START)\n",
    "                        posts = get_posts(USERNAME, ROOT, get_before=END, get_after=START)\n",
    "                        articles = google_trends[(google_trends['Date'] >= START) & (google_trends['Date'] <= END)]\n",
    "                        \n",
    "                        start = time.time()\n",
    "                        tqdm.pandas(desc=\"Embed previous captions\")\n",
    "                        prev_posts[\"embeddings\"] = prev_posts[\"caption\"].progress_apply(vectorizer.transform)\n",
    "                        embedding = calc_embedding(prev_posts[\"embeddings\"])\n",
    "                        end = time.time()\n",
    "                        embed_captions_time = end - start \n",
    "                        \n",
    "                        start = time.time()\n",
    "                        tqdm.pandas(desc=\"Embed articles\")\n",
    "                        articles[\"embeddings\"] = articles[\"article_content\"].progress_apply(vectorizer.transform)\n",
    "                        end = time.time()\n",
    "                        embed_articles_time = end - start \n",
    "                        \n",
    "                        articles[\"similarity\"] = articles[\"embeddings\"].apply(\n",
    "                            lambda x: SIMILARITIES[SIMILARITY](embedding, x)[0][0]\n",
    "                        )\n",
    "                        \n",
    "                        # Get top posts per day and calculate jaccard \n",
    "                        top_k = articles.loc[articles.groupby(articles[\"Date\"])[\"similarity\"].nlargest(1).index.levels[1]]\n",
    "                        \n",
    "                        article_text = top_k['article_content'].str.cat(sep=' ')\n",
    "                        posts_text = posts['caption'].str.cat(sep=' ')\n",
    "                        jaccard_score = jaccard_similarity(article_text, posts_text)\n",
    "                        \n",
    "                        results.append({\n",
    "                            \"username\": USERNAME,\n",
    "                            \"method\": METHOD,\n",
    "                            \"n_per_date\": N_PER_DATE,\n",
    "                            \"similarity\": SIMILARITY,\n",
    "                            \"start\": START,\n",
    "                            \"end\": END,\n",
    "                            \"vectorization_time\": vectorization_time,\n",
    "                            \"embed_articles_time\": embed_articles_time,\n",
    "                            \"embed_captions_time\": embed_captions_time,\n",
    "                            \"jaccard\": jaccard_score\n",
    "                        })\n",
    "                        pd.DataFrame(results).to_csv(\"FINAL_RESULTS.csv\")\n",
    "                        print(results[-1])\n",
    "                    except Exception as E:\n",
    "                        print(f\"Failed to calculate Jaccard: {E}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
